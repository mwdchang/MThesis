%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% This chapter covers the design aspects, primarily on visual design and
% touch interactions
% 
% - Should probably talk about active versus inactive versus context objects 
%   in the 3D visualization section
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Designing D-NPR}
This chapter covers our visual design process and our rationals for our
decisions. Our main interface, as shown in \ref{figure:overview}, is composed
of four major components:

\begin{itemize} [noitemsep]
  \item Stylized Visualization: The central point of the visualization system is
  a stylized rendering representing the physical entities in the text documents,
  the visualization can be zoomed and rotated to explore different viewing
  perspectives. Each entity is rendered with respect to a function which denotes
  its importance, we explore different rendering styles to take advantage of
  viewers' preattentive perception, allowing them to quickly uncover the
  important subject matters.

   \item Lens Widget: The lens widget is a detail-on-demand tool. It is used
   to specify spatial regions on the \threed visualization. Entities under the
   specified space are considered to be in focus, and more information about
   these entities are shown beside the lens as heatmaps.
%  \item Lens Widget: The lens widget is used for exploration and
%  visually querying the visualization. When the lens is hovered over the
%  \threed visualization, additional information of the objects underneath the
%  lens are displayed as heatmap charts placed around the lens circumference.

   \item Heatmap Widget: The heatmap displays time series data at the lowest
   granularity level in order to provide trend and pattern analysis. It is
   organized into a grid, each cell is shaded in accordance with its score of
   that time period. In addition, the heatmap identifies the entity name and its
   raw numerical scores.
   
%  \item Heatmap Widget: The heatmap widget displays time-series data, it
%  is similar in function to the histogram of the Time Filter widget, but
%  at a component level. Volume for each period is encoded as a shaded square and
%  arranged into a grid pattern to match the selected time range. The heatmap
%  offers the viewer several different perspectives to see trends and find outliers in the data.
  
  \item Document Widget: The document widget displays the source text documents.
  The panel displays documents relating to the current selected objects and 
  highlights all component keywords.
\end{itemize}

In addition, we introduce two domain specific widgets, which are used to filter
vehicle specific data into manageable pieces:

\begin{itemize} [noitemsep]
  \item Time Filter: The filter is composed of two independent range sliders
  with different granularity: year and month. The filter allows viewers to
  adjust the time range for the visualization. Accompanying each range slider is
  a histogram showing the accumulated volume of complains over a time period.
  The widget itself is place at the top-left of the display space.
  
  \item Hierarchy Filter: The hierarchy filters are placed at the
  top portion of the display similar to a series of drop-down menus or spinners.
  These are designed specifically for our data of automotive complaints. The
  hierarchy filters allow successive query refinement by organizational
  hierarchies of manufacturers, going in the order of : Vehicle Manufacturer,
  Vehicle Make, Vehicle Model and Vehicle Year. Hierarchy filter is also used to
  select two different vehicle types in comparison mode.
\end{itemize}


    % === Figure ===
	\begin{figure}
	 \centering  
	 \includegraphics[width=\columnwidth]{overview.png}
	 \caption{System Overview}
	 \label{figure:overview}
	\end{figure}
	% ==============

In the following subsections we will describe each visualization components in
more detail, the trade offs and choices we made and our justifications.


\section{3D Visualization}
This section describes how to perceive the \threed visualization, how it was
designed and discusses various design trade-offs.

\subsection{Rationale}
A major part of our design for this thesis is the mapping of abstract semantics
onto realistic looking, \threed models. But this can also be a source of
complication, we have to deal with the additional difficulties of navigating in
three dimensional space,  as well as work around perceptual limitations. So why
use \threed models in the first place? Familiarity with the models and varied
exploration methods were key motivations. The familiarity with how the physical
entities look in real life means people do not have to learn additional visual
mappings. Communication may be easier because there is a shared common ground
among the different parties involved. Proximal relationships are also easily
perceived if they have realistic spatial mappings and can encourage more
thorough exploration of the dataset. These advantages are not readily present in
abstract representations, while with careful design it is possible to overcome
many of the pitfalls of perceiving \threed graphics. Lastly, we are not merely
mimicking \threed objects, our visualization goes further to facilitate user
oriented tasks~\cite{Shneiderman2003}. We enhanced the rendering process with
NPR techniques, along with interactions to explore the \threed space in an intuitive manner. 

Yet another argument is that the use of a set of \twod images can also convey
realism, our opinion is that they lack the expressive power and playfulness of a
fully rendered \threed model. Flat image representation would likely result in
multiple images, used to cover different viewing perspectives, this can result
in more cognitive load due to the viewers switching between images to see 
different data.


\subsection{Visual Mapping}
Because our visualization environment makes use of 3 dimensional space,
extra care are taken into account for the selection of visual variables. Not all 
variables are appropriate, shape, position and orientation are inherently used 
to represent the geometries on the virtual model, a double encoding of these
variables is likely going to lead to confusion, compromising the ability to
interpret the visualization, or destroy any resemblance of the virtual model to
its real world counterpart. Thus these are rejected in the early part of our
design. Size is an interesting one, in theory size can support most of the
characteristics. However it is implied that all the objects of the same value have the equal size, 
which is certainly not the case for physical objects with sub components. Colour 
and value variables are not used to represent the virtual model, in the sense that 
they are not a part of the  basic geometry building blocks of vertices, line-segments 
and polygons. We found colour and value to be appropriate for our visualization, 
while they do not provide a sense of quantitative measure, the capability to see order, 
selective and associative characteristics makes them a logical choice for an
overview visualization.

Lastly, textures present an interesting option, textures can carry additional 
characteristics, particularly descriptive attributes. It may be possible to use 
textures to simulate certain effects, for example a rusty surface. Nonetheless,
textures do not possess order or quantifiable characteristics and was not
included in our design. However it may be interesting to use textures for
visualizing individual document, we leave this idea as future work.

\subsection{Stylized Visualization}
%Data comes in many different dimensions, include those with spatial attributes
%and those with abstract attributes. Our innovation is to show these spatial 
%vocabulary as they would exist in the physical world, while using the abstract 
%attributes as rendering parameters. For tasks related to word frequencies and 
%other scores, we hypothesize that people will be able to see, and communicate 
%better than in raw text formats because they are working with a familiar form. 
%Proximity-based clusters formed in the spatial dimension would also encourage 
%exploration of these areas that may otherwise go undetected. In our visualization, 
%we use \threed models comprised of segmented sub-mesh groups that link to our
%part-of relation hierarchy of component keywords.

In order to enhance the message carrying capacity of the visualization, we
considered several NPR techniques as a medium to created a more expressive
illustrations. We associate the entity score with either a NPR technique, or use
the score as a parameter into a NPR function. In accordance with our
requirement, the encoding scheme should be clear and distinguishable by visual
examination (R1) while maintaining the real world aspects. Our effects include
varying stroke, halo/glow, colour variations, and transparency effects.

We chose colour mappings as our primary visual encoding which denotes the
strength of non-zero score entities, and use other techniques to denote
selection and overall context.


Designing a colour scheme for the encoding of the virtual component objects
presented several design trade-offs. We colour each object by mapping its score 
to a linear diverging yellow-orange-red hue scale, which is further divided into 
six discrete scoring bins. While this setup has a limited granularity, it is
easier to perceive a small number of discrete colours than viewing a continuous
scale. We mitigate any ambigiouties that may arise with the discrete scale by
providing numerical figures with the lens and heatmap widget, which we discuss
further down.

% == Removed from colour scheme: explanation is weird, and perhaps obvious
%
%This has both advantages and disadvantages. The
%advantage is that it is much easier to distinguish a small number of hues than a
%continuous hue scale. The disadvantage being that we lose the ability to compare
%the relative distance between the buckets. For example, imagining the case were
%we have low value in bucket 1 and high value in bucket 2, this is equivalent
%to high value in bucket 1 and low value in bucket 2. We partially mitigate
%the ambiguity by allowing the viewers to extract exact numerical values with the
%heatmap widgets, we will discuss this further down. We display the discrete hue
%scale as an on-screen legend located near the top of the display.

\threed geometries may not be visible due to occlusion or containment.
The first case can be partially solved by altering
the viewing distance and viewing perspective on the visualization, whereas in
the second case no amount of viewing adjustment will solve the occlusion issue.
To address this problem, we double-encoded the importance score as both the
colour and transparency values. The transparency value of each object is
proportional to the object’s score, such that the higher scored entities appear
more opaque, while the lower scored entities appear more transparent. The
maximum and minimum transparency values are capped between 0.4 and 0.8 such that
no objects are completely opaque or completely transparent. Our transparency scale
is slightly weighted to give more emphasis for more frequently occurring
entities. One challenge of rendering translucent geometries in \threed space is
that ordering of geometries becomes important for blending to work correctly,
out-of-sequence geometries appear to lose their depth cues when blended
together. We discuss this effect, and solutions in further detail in the
implementation chapter.


The zero score has a special semantic in the visualization. When an entity's
score is zero, this indicates that there are no known references of the entity
in the documents. Not rendering them would reduce visual clutter, but comes at a
cost of not having a background reference, which gives visual cues to not only
what viewers are looking at, but also the placement and relative position among
other entity objects. To show that these components are semantically different
than others, we rendering them in silhouette styled edges with a just
noticeable colour so they are visible but not overly
distracting~\cite{BAR2007a}. It is important to note that 0 scored objects only
provides graphical context, they do not partake in any user interactions.




\subsection{Selection}
Selection of entities is used to refine the visualization scope, for example
selecting the windows entity tells the system to only visualize documents that
refer to windows. The system provides several methods for selecting entities,
selections can be made by directly interacting with the \threed visualization,
or through interaction with the heatmap widget.


By default the application has no entities selected, thus the visualization
reflects the absolute number of occurrences of each entity. As
selections are made, each entity’s score is recomputed to show co-occurrence
relations with the selection, note since selected objects
fully co-occurs with themselves, they are promoted to the highest bin.
In this manner, high correlations are red and highly opaque, while low
correlations are yellow and highly transparent. For example, if we select
the windshield wiper, the visualization is rerendered to show entities
that co-occur with the wiper component and the strength of this
relation. If we also select the windows, the visualization will show
co-occurring relation to both windshield wiper and windows. This example
is reflected in Figure \ref{figure:???}. The rerendering process is facilitated
by an animated transition that interpolates the graphical effects.


% Entity objects can be acquired in two ways: One can directly select the entity
% by performing the tap gesture on the \threed representation, or tapping the
% heatmap through the lens widget. In the case where objects occlude each other
% in \threed space, direct selection will return the object that is closest to
% viewing point. In order to select occluded objects, the scene can be
% rotated to reveal hidden entities. Alternatively, we can use the lens’ depth
% functionality to cut away occluding geometries, or directly clicking on the
% heatmap representations which are always available.
% 
%     % === Figure ===
% 	\begin{figure}
% 	 \centering  
% 	 \includegraphics[width=\columnwidth]{interaction_select.png}
% 	 \caption[Interactive Selection]{The visualization changes based on the
% 	 currently selected entities.}
% 	 \label{figure:selection}
% 	\end{figure}
% 	% ==============
% 
% Selection is toggle based. When an entity is selected, we apply a blue borderb
% to the associated heatmap and the line segments that links the heatmap back to
% the visualization. To visually accentuate the selection in \threed
% space, we employ a halo like effect around the hull of the mesh objects. Both
% selection and deselection of any entity triggers an event to recalculate the
% entity scores, an animated sequence then starts to interpolate the colour of
% each \threed object to reflect the scoring change.
% 
% The re-evaluation of the entity scores are based on current selection,
% specifically they are recalculated based on their co-occurrence relation with
% the current selection. Thus, highly co-occurring entities will score higher and
% be more visible than lowly co-occurring entities.
% 
% Selection of an entity with zero score is considered invalid, instead, the
% system will propagate the selection event through the zero scored entities.

 

\subsection{Trade-offs}
We recognize that blending different hues in \threed space does not necessarily
produce a result which preserves the original hues, and can potentially lead to
distracting visual artifacts. Different hue preservation schemes
exist\cite{Chuang2009} but were not implemented for this prototype due to the
added performance complexity (Hue adjustments are done at per pixel levels). Subjectively, 
we did not find any visual distractors and thus decided that this was not necessary. A 
single-hue scheme with varying saturation and opacity was tried, but we found it less 
eye-pleasing and lacks the \emph{pop-out} effect that is more prominent on multi-hue 
colour schemes. 


%Due to known issues with blending, we have tried to use a single hue grey scale 
%with varying brightness, but we found that it was somewhat difficult to distinguish 
%overlapping or contained objects. Subjectively speaking, single-hue also looks
%less aesthetically pleasing, the entities did not have the ``pop-out'' effect
%as we seen with multi-hue schemes. Thus we decided that using  a multi-hue
%scheme was more appropriate, despite the potential blending artifacts.

A second design trade-off was whether lighting effects should be used. Lighting
effects such as specular lighting can create distractions because it can create 
highlights in places of little or no significance. Without any realistic
or simulated lighting effects, the visible colour of the components exactly
matches the colour assigned to the score and as seen on the legend.
However, without any sort of lighting, particularly some type of diffuse lighting, 
the \threed nature of the model, and the details of various components are not
sufficiently visible. Adjacent objects that share the same score appear to be glued together 
as a single component, adding boundary outlines helps but creates undesirable
visual clutters. When lighting effects are enabled, the objects are easily
distinguishable as lighting provides a clear silhouette. However, this type of lighting modifies 
the colour based on the incidence angle of the light rays, thus it no longer matches 
the assigned colour. Ultimately, we decided that the benefits of objects recognition and
familiarity outweighs the colour offsets. The results of our design choices can be seen in
Figure \ref{figure:visualEncoding}.

%Ultimately our design decision is that object recognition and 
%familiarity is important to us. Thus, by restricting the number of hues (6 buckets) 
%and using soft white light, we contend that the lighting effects do not disturb 
%the colour perception enough to obscure which hue-bin the component belongs to. 
%We made the design decision that the benefits of enabling lighting to visually 
%resolve the components outweighed the negative effect of shifting colours away 
%from those displayed in the onscreen legend. The results of our design choices
%can be seen in Figure \ref{figure:visualEncoding}.
 

    % ===== Figure =====
	\begin{figure}
	   \centering  
	   \includegraphics[width=\columnwidth]{visual_encoding.png}
	   \caption{Showing the visual design trade offs}
	   \label{figure:visualEncoding}
	\end{figure}
	% ==================


Since we are dealing with \threed geometry, it can be tempting to apply other
types of techniques. For example we attempted to encode the importance and other 
numerical semantics into a geometrical distortion function that can be applied
directly onto a \threed mesh. In practice, this does not work well for visual evaluation:
in general, objects are of different shapes and sizes, applying a small distortion 
is not entirely obvious while a large distortion can destroy the familiarity of 
the form. It is also more difficult to recognize objects under distortion and to 
compare the degree of distortion among objects.


%Yet another problem is that it is not possible to order or quantify by 
%shape \cite{BER1983a}, which makes the distortion a qualitative measure instead
%of a quantitative one. In addition, the ability to read quantitative value from a 
%distortion field presumes that the viewer has a mental model of what the object 
%looks like without any distortions, an assumption that we cannot make with our 
%intended audiences.  However distortion remains an interesting possibility because 
%we may use distortion to visually paint the action words, testing this will be 
%part of our future work.



\section{Lens Widget}
Using a metaphor of looking through a magnifying glass to reveal better details
about a specific subject, we created an interactive widget to extract and show 
detailed information about entities in the text documents. With respect to the information
seeking process, this approach combines both filter and detail-on-demand phases.

The lens widget operates in a hybrid \twod/\threed space, the lens itself exists
on a flat \twod plane, it casts a cylindrical query volume into the scene. When the
lens is positioned over the visualization, each entity object is tested to see if their
corresponding meshes are in the lens' querying volume. We use the centroid to do the inclusion
test, though more accurate, albeit slower methods exist~\cite{Ali2005}. Entities that are
under the lens' area of focus have detailed information, shown as heatmaps on the left/right
side of the lens widget. To create association between \threed entities and heatmaps, we connect 
them together with line segments: first the heatmap is connected back to the lens' circumference,
then from the circumference back to the entity's projected centroid.


% In this project, we use the
% centroid of the mesh as our test criteria, more advanced techniques do exist
% and would make detection more accurate \cite{Ali2005}, though this comes with
% higher performance implications. Entity objects in the lens activate their
% heatmap charts, which are displayed alongside the widget's circumferences in a
% flush-right/flush-left manner. To make the associate between the \threed mesh
% and the heatmap chart, we make use of line segments to connect the chart back to
% the entity's centroid location in screen space.

  
    % ===== Figure =====
	\begin{figure}
	 \centering  
	 \includegraphics[width=\columnwidth]{lens_schematic.png}
	 \caption[Lens Widget Schematics]{Cross section schematics of the Lens Widget}
	 \label{figure:lensSchematic}
	\end{figure}
	% ==================


    % ===== Figure =====
	\begin{figure}
	 \centering  
	 \includegraphics[width=\columnwidth]{lens_example.png}
	 \caption[Lens Widget in Action]{The lens widget can be tuned to expose
	 occluded geometries. Left shows the unaltered geometry. On the right, the
	 lens cuts into the geometric shapes to expose the tree contained in the
	 cylinder, which itself is contained in a box.}
	 \label{figure:lensExample}
	\end{figure}
	% ==================
	
	
The lens widget utilizes its own rendering pipeline, object geometries
are sent into the pipeline as normal, the rasterized result is
then stored in an intermediate buffer and later combined in fragment
shaders with the default rasterization scene. This is an independent
process, and thus allows us to render the lens’ scene in different rendering
styles and semantics. To visualize the lens widget itself, we
draw a semi-transparent border around its circumference so viewers
are aware of its existence. When interacting with the lens widget, the
widget is active and we render the border in blue, otherwise we use
the default grey colour. The semantics of the lens is not impacted by
whether the lens is active or inactive.	

% The lens widget utilizes its own rendering pipeline, object geometries are
% sent into the pipeline as normal, the rasterized result is then stored in an
% intermediate buffer and later combined in fragment shaders with the default
% rasterization scene. This is an independent process, and thus allows us to
% render the lens' scene in different rendering styles and semantics. To visualize
% the lens widget itself, we draw a semi-transparent border around its
% circumference so viewers are aware of its existence. When interacting with the
% lens widget, the widget is active and we render the border in blue, otherwise we
% use the default grey colour. The semantics of the lens is not impacted by
% whether the lens is active or inactive.


The lens enables three different actions. The position of the lens
can be moved by dragging within the lens, impacting the currently
focused entities and the heatmap charts. The lens can be resized by
dragging on the border of the lens, increasing or decreasing the query
area. Lastly, the depth plane can be adjusted by rotating the depth
selector tab around the circumference of the lens (see Figure \ref{figure:xxx}). The
depth plane function provides a method for people to reduce occlusion,
as all entities that are the cut by the plane are drawn in an outline style,
allowing viewers to see through them and into the object. Objects that
are cut off are excluded from any scoring calculations, they also have
their heatmaps hidden to reduce visual clutter. These three interactions
can be combined together to create a rich, flexible query mechanism

%Three different types of interactions can be performs on the lens widget. The
%position of the lens can be moved around the display, which impacts the
%currently selected filters and the heatmap charts. The lens can be resized,
%which increases or decreases its querying area. Lastly, a depth field can be
%adjusted, this field acts as a cutting plane that removes entity objects that
%comes before the plane. The depth field function provides a method for people to
%reduce occlusion, as all entities that are the cut by the plane drawn in
%an outline style that allows viewers to see into the object. Objects that are
%cut off are excluded from any scoring calculations, they also have their
%heatmaps hidden to reduce visual clutter. These three interactions can be
%combined together to create a rich, flexible query mechanism.

Multiple lens widget are allowed for simultaneous explorations of
different parts of the visualization. For example, if the subject matter is of
an elongated shape, it is possible to use two lens to explore the entities
positioned at either end. However, we have not defined any semantics for
multiple lens to co-exist in the same spatial location, that is, the lens widget
has no defined behaviour when it is overlapped with another lens.

%To build a rich, flexible query mechanism, the position of the lens and the
%radius can be altered, thus creating different querying volume in different 
%spatial locations. In addition, each lens widget use a depth parameter that acts 
%as a cutting plane, all objects that comes before the cutting plane are excluded 
%from the tagging process and are rendered as inactive objects. The depth parameter 
%allows people to investigate occluded objects without resorting to changing their 
%viewing perspectives.

%Multiple lens widget are allowed to facilitate simultaneous exploration of
%different areas, for example if we are looking at a tower structure, we can one
%lens examining the base of the structure, while another lens examining the top of
%the tower. However we have not defined any semantics for the lens to overlap, thus 
%each lens should be used individually.

\subsection{Spatial Interaction}
Traditional systems, particularly databases use explicit queries as a mean to
communicate with the underlying data. While this works well for task analysis,
it has an implicit assumption that the person knows something about how the
system works, and how the data is structured. Thus it can be a limiting factor
that prevents a wider audience from using applications without prior training.

Here we take a different approach, the lens widget allows people to demand and
filter detailed information by means of a visual query. Unlike explicit queries,
visual query is done more passively by moving the lens widget about the screen
to inspect different spatial areas that appear interesting to the viewer. This
is the key difference, as the focus selection is done visually, rather than
through explicit selection, this means the viewers do not need to know what the
entities are called to select them, reducing the need to have prior knowledge
about the data. This type of interaction is more playful and open-ended, people
are free to explore the visualization on their on terms which may lead to
unknown or unexpected discoveries.

In addition of the freedom to explore the visualization, the spatial
interaction has two other advantages: proximal relationship and reduced effort
in query construction. First, with most complex objects, incidents relating to
an entity is rarely in isolation, neighbouring objects are often impacted as
well. The lens widget allows selection of a volume range, which easily
facilitate bringing back entities close to the one we want to focus on. Second,
the visual query allows us to form queries that can be difficult to construct in
conventional systems. Imaging a case where we want to know how many vehicle
complaints are related to the ``front'' of the vehicle, traditional query would
require knowledge of the parts that belong to the front of the car, then follow
by an aggregation step of constructing a query that includes all these parts.
Our visual query simplifies this step by taking advantage of the familiar form
factor, a lens widget can be over the ``front'' of the vehicle visualization,
automatically pulling out all the relevant entities.



\section{Heatmap Widget}
The heatmap is an interactive widget that shows entity specific information over
the selected time frame. Each heatmap is designed to communicate how the volume
of complaints changed over time for individual entity components by fitting a
time series data onto a two dimensional grid. In particular, the heatmaps allows
year-to-year and month-to-month comparisons.

%The information chart extruded from the lens widget is designed as a
% heatmap-like chart, it shows the volume of complaints registered against individual objects 
%across time. Like our design for the time slider histograms, our intention is to 
%allow people to compare specific periods of time across different years for a 
%specific objects, albeit in finer detail. 

Prior to our heatmap implementation, we have also considered using a simple 
scatter-plot approach, with time on the X-axis and the volume of complaints on
the Y-axis. While this solution is simpler to read and likely easier to detect
long term trend, its major drawback is that it is likely more difficult to
make seasonal comparisons. For example, imagine a case where we want to compare
summer to fall over a 4 year span, one must constantly make the context switch
to decipher which parts of the graph represents summer, and which parts
represent winter. Also, in extreme cases scrolling need to be utilized for very
long time frames.

We arrange the time segments chronologically onto a grid like a calendar,
the month are arranged left-to-right in ascending order, and the years arranged 
top-to-bottom in ascending order. Each cell then represents the entity score for
the particular month. We use the same 6-bin colour encoding for the heatmap
widget to keep a consistent colouring scheme throughout the system. The widget
label, from left-to-right, shows the entity name, the number of complaints
referring to the entity with respect to the current query, and the number
of complaints in total if nothing is selected. Due to the use of synsets, each
entity has at least one corresponding string representations, we use the first
lemma that appears in our vocabulary, which may differ from actual text.

When examining the heatmap, trends and outliers can be detected visually. The
grid view aligns both month and year time dimensions, allowing viewers to
compare year and month with relative ease. Hovering over individual cells will cause a
tooltip to appear, showing the precise numerical score for the month, the tooltip itself is 
rendered with a semi-transparent style to avoid occlusion. Hovering also creates 
a visual cue of a highlighted border around the selected cell, the highlighting
effect is carried across all visible heatmaps as a brush+link effect.

With respect to the actual placement of the heatmaps, we have considered two
types of labeling layout for our lens widget: A flush-left/flush-right layout
that places the heatmaps on either left or right side of the lens, and a radial
layout where each heatmap is placed around the lens’ circumference. The radial
layout is anchored at the centre of the lens, imaginary line segments extends
from the centre through the projected centroids, and a real line segments
extends the imaginary line to the circumference. The result was eye-pleasing,
but due to the nature of the lens being an exploratory tool which is moved
around the screen frequently, the layout is unstable and not suited for this
particular usage case. For the flush-based layout, we first sort the object
centroids by their Y-coordinates in screen space, then we place the heatmaps on
left/right side based on the heuristics below. Since there is no reliance on the
centre of the lens for placement, movement of the lens widget will not cause
drastic changes to the heatmap placement and thus produce a more stable looking
layout. Lastly we horizontally align the charts so it is easier for people to
compare values across different heatmaps.

In order to avoid occluding the \threed visualization, or to run off screen
space, we employ the following heuristics for heatmap placement.
\begin{itemize}[noitemsep]
  \item Heatmap placement should always be outside of the axis-aligned bounding
  box (AABB) in projected screen space of the \threed visualization, unless the
  AABB itself runs off the screen.
  \item If the heatmaps are off the screen space, perform contraction to move it
  back into screen space.
\end{itemize} 

Because of limited screen space, we have made the decision to cap the number
of heatmaps that can be displayed at any given time. Rather than showing
heatmaps for all the entities under the lens widget, which would
inevitably create visual clutter, we employed a scrolling mechanism that 
allows people to scroll through the entities under the lens. We have a parameter
N which controls the maximum number of visible heatmaps. We have no
recommendations to what N should be, as it is tied to the amount of entities in
total, as well as the spatial placement of these entities. We found N=8 works
well with our vehicle complaint corpus.


\section{Document Widget}
The document widget is the final stage of our drill-down process by providing links 
to the raw text (Requirement R4). Each document is divided into two sections,
the header section shows each document’s fixed attributes and the content
section shows the raw text descriptions. We denote the selected entity words and
co-occurring entity words using blue and grey highlights respectively to create
contract against the remainder of the text. Scrolling is enabled when text
content overflows the display panel. A document widget in action is seen in
Figure \ref{figure:document}.

    % ===== Figure =====
	\begin{figure}
	 \centering  
	 \includegraphics[width=\columnwidth]{document.png}
	 \caption[Document Widget]{The document viewer, the words highlighted in blue
	 are selected. The words highlighted in grey are the co-occurring entities}
	 \label{figure:document}
	\end{figure}
    % ===================

We restrict the application to have a single document widget only. The widget is
toggle-based and is by default hidden from view. Once activated, an animation
will expand its dimension from a single pixel to its full size at the
activation coordinate, a reverse animation is used to deactivate the panel. 

Once fully visible, the document widget embed itself with two different
interaction regions. The left region, which takes up 80 percent of the panel, is
used for relocating the document widget to a different location. The right
region is designated for scrolling through the document text.





\section{Data Filters}
In this section, we describe the time and hierarchy widgets. These are used to
model the fixed fields (date, make, model, etc.) in the complaint documents.
They provide domain specific functions to filter the dataset into logical
partitions. See Figure \ref{figure:filterFull} for a close up of our data
filters.

    % ===== Figure =====
	\begin{figure}
	 \centering  
	 \includegraphics[width=\columnwidth]{filter_full.png}
	 \caption[Interactive Filters]{Time Slider Widget and Hierarchy Widget in
	 interactive mode}
	 \label{figure:filterFull}
	\end{figure}
    % ===================
    

    % ===== Figure =====
	\begin{figure}
	 \centering  
	 \includegraphics[width=\columnwidth]{filter_compact.png}
	 \caption[Compact Filters]{Time Slider Widget and Hierarchy Widget in compact
	 mode}
	 \label{figure:filterCompact}
	\end{figure}
    % ===================
    
\subsection{Hierarchy Filter Widget}
The hierarchy widget is designed to model inclusive relationship, in particular,
it is specifically designed to address the need to for comparison(R2) and trend
finding(R3). For our problem domain, this relationship is represented as the
organizational hierarchy. Our data contains four such fields: manufacturer,
make, model and model year. For example: Civic (Make) belongs to Honda
(Manufacturer). These filters are shown as a variant of the combo boxes which
supports single selection, each item in the filter shows the name and the number
of documents associated with this organizational level. Rather than having the
readers comparing items by reading the numbers in text format, we double-encoded
the number of document as a horizontal histogram in similar style as the scented
widget approach in \cite{Willett2007}. The histogram bars are shaded in light
grey so they are visible but not overly distracting. The histogram makes it very
clear any outlier items, and allows reader to make relative comparison among
items. We capped the number of items that can be displayed at once, as such,
scrolling is required to navigate long list of items. When an item is selected,
we change the selected item's font type to bold and change its bar to the
selected colour(blue).

Each level of the hierarchy is shown in individual filters. We position the
filters left-to-right across the display space, from the most general to most
specific classification. Each filter's content is 
dependent on the selection made on its parent. For example, the ``make''
selection widget will contain different makes if ``GM'' is the selected manufacturer than
it would for ``Chrysler''.  Non-top level hierarchy widgets remain hidden from view 
until it has selectable content, thus at the start, only the top level 
(manufacturer) filter widget is visible.

The hierarchical content is dependent on the currently selected time. Our
application queries the document collection and reconstructs the hierarchy. Previous 
selected items are persisted if the items are available within the new selection 
criteria. In the case where the items do not exist, the widgets, and all their 
children widgets are hidden from view and we clear the corresponding selections.

\subsection{Time Filter Widget}
The time dimension is encoded as a histogram, with the height of each bar denoting 
the volume of unique complaints for that time period. There are several granularity 
options with our document collection: daily, weekly, monthly or yearly. From a 
preliminary analysis of the incoming volume of complaint documents, we found that 
daily and weekly granularity resulted in too much noises, there are insufficient
amount of data at that level to form visually detectable trends and other
interesting patterns. Thus we selected to use month and year as our base time
periods. The widget is made up of two sliders. The top slider represents time
period in years, and the bottom slider represent time periods in months. Text
labels at the top of each bar give the numerical representation of the volume of
documents.

Time selection is not continuous, but rather in a block like manner. Multiple
years and multiple months can be selected simultaneously; when more than one
year is selected, the values on the month's histogram become the accumulated sum
of the month across the selected years. There were two motivations for this
design approach: One, this allows us to analyze a large time frame without
sacrificing too much screen real estate. Two, because the number of months is
static, viewers can compare month-to-month or season-to-season without
difficulty,  this follows up on our requirement to allow analysis of seasonal
trends (Requirement R2). We illustrate with an example, suppose we want to
compare the number of incidents in January versus June from 2000 to 2005, we
select 2000 to 2005 on the year slider, we can then compare the January volume
and June volume directly, furthermore, each month can be selected for further
analysis.

User interaction is done through click/tapping the individual histogram bars, or
by dragging each slider's respective markers. A short cut for selecting an
entire year is provided by double clicking/tapping any non-zero bar on the year
range slider. An animated transition is used to interpolate the height of the
histogram bars from their current volume to their new volume.
 


\subsection{Compact View}
We provided a compact panel that encapsulates the legend, time and hierarchy
widgets as a work around to create more screen spaces for the main
visualization. As seen in Figure \ref{figure:filterCompact}, the compact view
removes much of the interactive GUI elements and replaces them with textual
summaries placed across the top of the display. It allows viewers to zoom in
closer and place interactive widgets in spatial positions that would have
otherwise cause occlusion issues. The compact view is turned off by default to
first allow people to select their filtering criteria. As with all other
display changes, we use a smooth animation to facilitate the transition.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Touch Display}
Touch enabled system can be deployed in situations that is otherwise cumbersome
for systems that uses mouse/keyboard input. For example, in a
walk-up-and-use scenario in a public place, or within a meeting in an office
setting. Our visualization is designed for these settings where traditional input devices may
not be available, in particular our visualization system is suited for large
displays. In this section we describe how we came up with our touch interaction
designs.

% In this section we describe in greater detail how a person interacts with the
% visualization by changing the underlying filters and using the widget to create
% visually based queries. We also describe here our design for touch based
% gestures, and visual feedback system for help people in their orientation of the
% application.

%Interaction with the \threed model and interactive widgets dynamically update
%the visualization to reflect any changes. The change of visualization state is carried 
%through an animated transition that interpolates the colours and histogram values. 
%In this section we describe the high-level interactions and models that 
%support common tasks.

% This needs some work, it sounds kind of weird at places



%Selection is toggle based, thus clicking on a selected object or heatmap will 
%de-select that item. Clicking on an empty space will trigger a reset and de-select 
%all items. Selected items are accentuated visually with a glowing blue halo
%around the objects, this also applies to corresponding heatmaps which will have
%their border highlighted in blue. Any entities with 0 scores are considered
%inactive and are only drawn on the screen to give context, as such they are not
%selectable, any clicking on inactive object will cause the select action to go
%through the object to select the next nearest non-zero entity.


% This should be moved to the data section, probably. It talks about scores and
% how they are recalculated in co-occurring situations

%Selected objects are added to the selection buffer, which also act as a query
% filter.
%When selection buffer is not empty, object scores are calculated in relation to 
%the objects in the buffer, thus forming co-occurrence relations. For example, if 
%we selected the “engine” and “door” objects in the visualization, the rest of the 
%objects are recalculated to see if they co-occurs with both “engine” and “door”. 
%By this convention, the selected objects will always have the maximal score after 
%normalization, and will be coloured with the deepest shade on our colouring scale.


 



\subsection{Surface Based Interaction}
We use the idea of zone to segment our display space and to process touch
point events. Each zone consist of one or more polygonal spaces with predefined
semantics for handling touch based gestures. In the event that the zones overlap
each other and an event comes in, we only process the zone with the highest
priority. The priority of each zone is fixed and is predetermined. 

When a touch point is registered by the sensor hardware, we tie the touch point
to a single zone, the coupling will remain so until the touch point is removed.
The reason for this approach is to create a continuous interaction model, we
want to avoid sudden change of semantics which defies user expectations. This
approach also allows a subset of our dragging gestures(lens handle, slider
makers and scrolling) to continue to execute even if the actual touch point is
move off the predefined area, allowing higher tolerance for human error and
error recovery.

In the list below, we summarize the different zones we have in our application
\begin{itemize}[noitemsep]
  \item Visualization Zone: The main visualization, handles selection
  and deselection semantics of \threed objects, as well as heatmap selection.
  \item Time Zone: Covers the year and month time sliders 
  \item Filter Zone: Covers the organizational hierarchy filters
  \item Lens Zone: Handles semantics for change the physical attributes of a
  lens widget
  \item Document Zone: Covers the document widget
  \item Empty Zone: An empty zone is specially designated zone that is none of
  the above. Empty zone handles gestures related to camera and miscellaneous
  functions.
\end{itemize}


While we tried to adhere to commonly accepted gestures for navigational and
selection based tasks, our gesture design is also influenced heavily by our
hardware and our perceived usage scenario: an infrared sensor overlay placed
atop a large, nearly vertical display screen. Our hardware setup has several
design implications. The infrared sensor is imprecise because it senses
movements instead of real touches, as such it is possible to introduce
false positives due to how the hand posture and orientation. We can use software
heuristics to mitigate the consequences of these unintended noises, however,
there are ambiguous cases where we cannot guarantee the correct outcome. For example,
imagine a single handed gesture with the thumb and index finger, we have
noticed through experimental trials that the knuckles on the other fingers are
often picked up as extra touch points as well due to their proximity to the
sensor. In this case, it is difficult to judge which points are intentional and
which ones are not; these points are roughly equal distance apart, and without
additional information such as a camera image or depth which may be used to
conjecture hand orientation, we cannot guarantee correctness as we only have
Cartesian coordinates as our data source. Due to said reason, we decided
to abandon any complex, explicitly-singled-handed, multi-fingered gestures in
favour of a simpler, less error prone approach.

Our final gesture set is a cumulation of several iterations of redesign, new
heuristics and refinement of existing gestures. We pilot-test each iteration by
asking people to perform sample tasks and observer their reactions, in a few
cases we gave no instructions on how to use the system in order to observe their
natural instinct.

In our current iteration, we designed two types of touch gesture semantics for
our system: a tap semantic and a hold semantic. The tap semantic results from a
single touch where there is no delay between the initial touch gesture and
subsequent actions, conversely the hold semantic requires the user to hold the
initial touch point briefly before transitioning into subsequent movements. We
map the tap semantic to actions to selection, scrolling and orientation logic.
For the gestures that use the hold semantics, we reserve these gestures for the
toggling our custom made user interface widgets. 
We summarize our gesture set
below, group by by functional areas:
\begin{itemize} [noitemsep]
  \item Perspective Manipulation: Perspective manipulation consist of rotation
  of \threed model and camera zoom. Rotation is achieved with a
  single point horizontal or vertical drag gesture, which corresponds to
  rotation of the XY and XY planes. Zooming events are triggered by bringing
  together two touch points closer together or further apart. All perspective
  manipulations are restricted to the empty zone.

  \item Entity Selection: Entity selection is triggered with a single tap event.
  A hold gesture on an entity will invoke the tooltip widget, if applicable.
  
  \item Time Sliders: Single-tap gesture is used to send select events to
  individual time sliders. Dragging gesture is used to move the markers left and
  right for selecting multiple periods.
  
  \item Hierarchical Filters: A single tap gesture is used to open, close and to
  make selections. Scrolling is achieved by dragging the item list vertically.
  Note the gestures are associated with unique events, thus we do not allow
  transition from one semantic to another, even though the gesture point is on
  the same zone. For example, one cannot transition directly from dragging the
  item list to select, a separate gesture need to be executed to perform the
  selection.
  
  \item Lens Manipulation: A lens widget is created by specifying its diameter
  with two hold points, for example using index fingers on left and right hand
  to create the diameter. We impose a minimum and maximum diameter length to
  keep the physical size of the lens within reason, with our display hardware,
  we use the range between 100 to 500 pixels. Dragging gesture performed on the inner part
  of the lens shift the lens location, while dragging gesture performed on the
  border resize the lens with respect to the point's distance away from the
  lens centre. A resize that results in a diameter that falls below the minimum
  threshold removes the lens widget from the application. The handle on the
  widget adjusts the depth parameter, dragging the handle counter-clockwise
  increases the cutting depth, while the reverse decreases the cutting depth, we
  modelled this behaviour after the zooming mechanism on the barrels of camera
  lenses.
  
  \item Text Browsing: The document panel becomes active upon a hold gesture
  event, with no other hold events in its vicinity. The hide the panel from
  view, executing a hold gesture anywhere on the document widget will trigger
  the close event. The widget is further subdivided vertically into two separate
  zones at 80 percent and 20 percent of the widget's width. The larger of the
  two listens to move events, while the smaller one, positioned on the right
  side handles scrolling of text. Both of these zones receive tap based
  events.
  
\end{itemize}

    % === Figure ===
	\begin{figure}
	 \centering  
	 \includegraphics[width=\columnwidth]{feedback.png}
	 \caption[Visual Feedback]{Clockwise from top left: normal touch point
	 gestures, touch point in movement transition, unrecognized touches,
	 transition to hold gestures.}
	 \label{figure:feedback}
	\end{figure}
	% =============== 


\subsection{Visual Feedback}
When using the keyboard, mouse and other hardware peripherals, actions are
rewarded with some type of haptic feedback, for example we know when a key on a 
keyboard is pressed of depressed. This behaviour allows people to be more keenly 
aware of the system's current state. This is not true with touch interfaces,
with touch sensing technology, it is possible for touch points to become lost during a gesture, 
this is due to the users accidentally lifting their fingers. When this happens people 
can get confused because they may be not be aware that their touch points are lost 
since their fingers are still contacting the table, there are no feedback system 
to alert the actual touch point had disappeared. To accommodate the lack of
physical response, we implemented out own visual feedback system. We created
four different types of visual cues, which we summarize below and can be seen in
Figure \ref{figure:feedback}. 

\begin{itemize}[noitemsep]
  \item Touch Point: Whenever a touch point is registered, we render a gradient
  circle at the XY screen coordinate as detected by the infrared sensor. The
  radius of the circle is slightly larger than the average finger tip such that
  is is always visible, this is about 15 pixels on our display. The position of
  the gradient circle is updated in-sync with the sensor updates, and is removed
  when the touch point is rescinded. This visual cue gives viewers immediate
  feedback of the active touch points, which can be used to perform higher level
  actions.
  \item Hold Point: A hold point has the same basic visual cue as a touch point.
  A hold point starts off as a touch point, a ticking timer running in the
  background determines when the touch point transitions into a hold point. We
  visualize this timing sequence as a arc outside of the circle, the arc grows
  with an increasing central angle and opacity that are mapped to the amount of
  time required to become a hold point. A hold gesture is activated once the arc has travelled
  the entire circumference and lock down. An interruption during the transition
  phase will remove the animation, the gesture will return back to a normal
  touch point.
  \item Trails: Our system keeps track of previous updates for a single,
  uninterrupted point. We visualize up to the last 10 most recent updates as
  breadcrumb trails to serve as a reminder of what type of high level gesture is
  being performed. The XY coordinates of previous update points are drawn in
  smaller circles, similar to that of touch points.
  \item Removals: These visual cues are used to denote any removals sensed by
  the hardware device, these includes the active points sensed by our software
  and also any other ones that were rejected by our evaluating heuristics. This
  cue gives the viewers some sense of the hardware capabilities and
  deficiencies. We think this is useful as a learning device, viewers are able
  to see where they may inadvertently cause unwanted touch points and use this
  experience to adjust how they operate their gestures next time they interact
  with the system. We visualize this as circular outlines that decrease in
  opacity and radius with time, they are removed from the system when the radius
  reaches zero.
\end{itemize}
 
 






