%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Related Works
% * Expand Touch Surface subsection
% - Show a picture of Wordle (A section of the thesis maybe)
% - Show a picture of WordsEye
% - Probably want to quote Carpendale's work on Elastic Framework
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Literature Review}
There are primarily six different research areas that are relevant to our work. 
The fundamentals of information visualization, a process of transforming
data into interactive graphics forms the basis of our work on text
summarization. Section 2.2, Descriptive rendering, reviews several
text-to-scene applications. A discussion of why NPR is beneficial is next, followed by recent examples of
visualizations that combine abstract and concrete concepts. Next, we review
the role of using lens as a focus+context interaction technique. Finally, we
review in brief the designs for touch interfaces.
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Fundamentals}
Visualization revolves around the process of mapping and representing data or
concepts graphically such that they are visible to the human eye. Visualization, 
in theory,  takes advantage of the human perceptual system to rapidly understand
what the data is communicating to the readers. 

Foremost we want to recognize that human visual system is limited, we
can only perceive a small fraction of the entire spectrum of light, and of our
field of vision, only a small area is truly in focus. However, there is a small
set of visual properties that can be detected extremely rapidly (comparatively
to other sensory systems) and accurately by low-level perceptual system known as
preattentive processing \cite{WAR2004b}. In theory, this perceptual system
deals with the extraction of specific graphical features without the need to 
consciously focus attention on the details, allowing viewers to spot salient and 
outlier data in a single glimpse. For example, picking out a red circle 
out of a group of blue circles is considered instantaneous, barring any visual 
abnormalities. In terms of design, exploitation of preattentive processing can 
greatly increase the rate of comprehension and ease of use.

For establishing clear visual communication, it is crucial to have a set of
visual building blocks that can be grouped and arranged to convey information. Much of
the initial work came from the field of cartography, particularly with the ideas
of marks and visual variables from Bertin~\cite{BER1983a}. Bertin defined seven
variables, including position, size, shape, value, colour, orientation and
texture. Each visual variable can have certain characteristics that can be used
to determine which of the seven visual variables is most appropriate visual
representation. These characteristics include properties such as selective,
associative, quantitative, order and length. A summary table below outlines the
relationship between each variable and its attributes.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
 & Position & Size & Shape & Value & Colour & Orientation & Texture  \\  
\hline
\hline 
Selective &  Yes & Yes & Maybe & Yes & Yes & Yes & Yes \\ 
\hline
Associative &  Yes & Yes & Maybe & Yes & Yes & Yes & Yes \\
\hline
Quantitative &  Yes & Maybe & No & No & No & No & No \\
\hline 
Ordered &  Yes & Yes & No & Yes & No & No & No \\
\hline
%Length & Yes & Yes & Maybe & Yes & Yes & Yes & Yes \\
%\hline
\end{tabular}\caption{List of visual variables and
their characteristics}\label{table:VisualVariable}
\end{table}


With the building blocks in place, the next step is a formal process for
building a visualization system, there are several key
contributions in this area. The visualization pipeline from Card \etal
~\cite{Card1999} describes a step-by-step process of data manipulation and
mapping as follows:
\begin{itemize}[noitemsep]
  \item Analysis: Normalization of raw data for visualization
  \item Filter: Select subsections of data to be visualized
  \item Mapping: Find the appropriate visual representations
  \item Rendering: Transform data into image data
\end{itemize}
For an interactive visualization, these stages are not a linear, one-off
process. These stages of Filter, Mapping and Rendering form a repetitive loop
as the users change parameters to explore different parts of the visualization.
The next part is finding interesting information within the visual presentation,
it is often through exploration that enables users to find and extract
interesting data. The process of facilitating useful graphical-based exploration
is advocated in Shneiderman's ``information seeking
mantra"~\cite{Shneiderman1996} and can be broken down into these specific tasks:
\begin{itemize}[noitemsep]
  \item Overview: See and summarize the entire collection
  \item Zoom: Zoom in on items of interest
  \item Filter: Removal of uninteresting items
  \item Detail-on-Demand: Select item/group and get details when needed
  \item Relate: View how an item relates to others
  \item History: Keep a history of actions to support undo and redo
  \item Extract: Allow extraction of subsections
\end{itemize}
Each task can then be further broken down by different types of interaction
techniques. Careful consideration should also go into each task to determine if
it is actually appropriate for the data and problem at hand.



%\textbf{Summary:} In our system, we are mostly concerned with the exploration
% of data, taking advantage of people’s ability to see colour and proximal objects. We follow 
%Shneiderman’s information seeking mantra in spirit: we have no support for 
%history or extraction functions, though these could be interesting directions 
%for our future work. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Descriptive Rendering}
Descriptive illustration can be considered to be a sub field of computer
graphics, which focuses on static illustrations or an animated sequence of
images based on text input. It can be considered as a text-analytic tool because
it renders the text in a literal sense or through some kind of interpretive
representation. Systems that attempts to do literal renderings are concerned
with understanding of natural languages, whereas interpreted rendering systems
tend to involve user intentions. Here we look at several examples. 

Though strictly speaking not a text-to-scene application, the IBIS application
from Seligmann and Feiner renders a \threed scene based on a set of
user-provided rules \cite{Seligmann1991}. These rules specify which object and locations in the \threed scene have higher degrees of interest. The application itself evaluates these rules and determines the optimal viewing perspective, as well
the application adjust the lighting conditions as a way to draw attention to
specific details in the scene. To our knowledge this is one of the first systems
where a preconstructed scene can be modified by text input.
 

One of the most fully-realized text-to-scene generation application is
WordsEye~\cite{Coyne2001}. WordsEye uses a combination of grammatical rules 
and heuristics to determine subjects and their actions. These subjects are
mapped against a large repository of \threed models, and the verb actions are mapped to 
predefined poses. One of the interesting WordsEye strives for is the preservation 
of spatial relations among subjects in the text, for example: ``The cat is on
the table. The cup is on the cat.'' will render a cup on top of a cat on top
of a table (See Figure \ref{figure:wordseye}). However, WordsEye is limited by
the inherent ambiguities in written language and the accuracy of its semantic interpretation. WordsEye also works best for
a single document, and is not meant for collections of text.

    % === Figure ===  
	\begin{figure}
	 \centering  
	 \includegraphics[width=\columnwidth]{wordseye.jpg}  
	 %\includegraphics[scale=1.0]{wordseye.jpg}  
	 \caption[WordsEye]{WordsEye rendering of ``The cat is on the table. The cup is
	 on the cat.''}
	 \label{figure:wordseye}
	\end{figure} 
	% ==============


Another example of text-to-scene generation is CarSim\cite{Akerberg2003},
which uses a similar logical processing and rendering process, but is
specifically designed to deal with the recreation of car accidents by parsing
traffic accident reports. Where CarSim differs is that it tries to generate an
animated sequence by inferring the speed and direction of vehicles from the text
content, it is less concerned with the literal representation of the car, but
with the situation surrounding the incident.

    % === Figure === 
	\begin{figure}
	 \centering  
	 \includegraphics[width=\columnwidth]{copyright/wordle.png}  
	 \caption[Wordle]{A typical Wordle layout~\cite{VIE2009a}. \copyright 2009,
	 IEEE. Reprinted with permission.}
	 \label{figure:wordle}
	\end{figure} 
	% ==============
 
 

%So far, the discussion have been around examples where the renderings are based
%on the literal meaning of the text. On the other side are interpretive
%renderings, where the images produced are not necessarily the ground-truth of
%the text, but often produced with artistic flares and particular theme in mind.

On the interpretive side, a popular example is the web-based application called
%Wordle(http://www.wordle.net). Unlike the systems mentioned above, Wordle does
Wordle~\cite{VIE2009a} (See Figure \ref{figure:wordle}). Unlike previous systems
mentioned above, Wordle does not perform any semantic inferences on the underlying text, it rather collects frequency
statistics of each word tokens and encode the frequencies as font sizes.
Unlike the closely related tag cloud, Wordle offers more creative freedom in
varying typography and colours, which are all interactively chosen by the user.
Thus it is possible to create interpretive art works based on social and
cultural norms, for example using the green colour to represent topics about
money and finance. Later variations (http://www.tagul.com) took this idea
further by binding the placement of the words to specific geometric shapes and
outlines, which further strengthens how people can create their own interpretations and
expose the underlying themes. For example, a text article about love can be
place inside a shape of a heart, instilling the idea that it is emotionally
based. However, because these creations are not usually linked to their
original text, it can be questionable whether these are proper tools for text
analysis. 
 

Another example is calligraphic packing \cite{Xu2007}, which takes a more
liberal, artistic approach. This research visualizes single words by distorting
individual glyphs to fit into \twod regions. Creative freedom is given to the
people to choose which image best represents the word, as well as the amount of
acceptable distortion. However, due to concentration on artistic style, the
legibility of the actual words after the distortion transformation is not guaranteed.


%\textbf{Summary:} Unlike WordsEye and CarSim, we start with a scene that is
%pre-constructed, that is, we know ahead of time the subjects in our text 
%documents and can place them into \threed space. While it is interesting to use 
%grammatical approach to grab context, we found it more generalizable to use a 
%dictionary approach similar to Wordle for parsing text because we iterate 
%over thousands of documents.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Non-Photorealistic Rendering}
Since the infancy of computer graphics, researchers have long been looking at
ways to render photorealistic images that are nearly
indistinguishable than that of a photograph taken by a camera. In recent years,
however, another area of computer graphics research have been looking in the
opposite direction: Non-Photorealistic Rendering. Non-Photorealistic Rendering can be 
described as any computer generated graphics that do not involve the accurate 
simulation of the behaviour of light. While photorealism has long been the holy
grail in graphics research, there are compelling reasons for using NPR
techniques. First, NPR images are judged based on their ability to
communicate a specific message to their viewers, thus they are driven to
emphasize features in a scene or some underlying attributes~\cite{Gooch2001}.
This is in contrast with photorealistic rendering where the goal is to compare
the rendering against a ground-truth image. Second, often times it is
undesirable to preserve a high degree of realism, reflections, shadows
and other natural lighting phenomena may obstruct or create
false-positive surface details. NPR can choose to ignore the natural laws of physics, and instead
choose to focus on different abstractions such that the illustration can be
meaningful to people from different fields. Because there are multitudes of
possibilities in NPR, this section solely focuses on prior works that related
to our work here, or served as our inspirations.

%\daniel{Perhaps repace toonshading with half-toning? more relevant because it
%mentiones textures} 

Images found in technically written materials, or that of
instructional manuals are quite different than those found in photo scrapbooks. The key point, as
stated above, is that communication is valued above realism. Early works by
Gooch \etal uncovered common themes in technical illustrations and ways to
automate some of these properties~\cite{Gooch1998}. Gooch \etal modified the
conventional shading algorithm to use a two-tone approach that shift from
warm-colours to cool-colours, in addition, they added edge lines and removed
shadows. Illustrations created in this fashion have major benefits over
photography. One, strong lighting effects under conventional shading model are
reduced, thus preserving the surface details under the light patches. Two, the
removal of shadow regions shows the hidden details that were not previous
visible. Other NPR shading techniques also exist, for example 
those that emulate the simplistic cartoon styles known as
cel-shading/toon-shading~\cite{Lake2000}. Generally speaking, cel-shading and
toon-shading map the intensity of light from a continuous function into discrete
values, creating distinct contours where the values change as if shaded by a
marker. Still, other works look at the simulation of physical materials,
for example textures as halftones to simulate stylistic artistic
sketches~\cite{Freudenberg2002}, and pen-and-ink
illustrations~\cite{Salisbury1994}. However it is semi-automatic as some human
input is required for the right combination of textures, normal maps and other
special effects.
   

Another major benefit of NPR is that it can be used to simplify or
accentuate geometric features. By taking advantage of visual perception
capabilities of seeing continuities and surfaces as dictated by Gestalt Principals
\cite{WAR2004b}, human can see and complete entire shapes with just a few
geometric primitives. Feature extraction is one way to achieve this goal by
simplifying an image down to geometric primitives of lines and curves.
Feature extraction can be done in either \twod or \threed spaces. The \twod
algorithm is image based, and based upon calculating the a discontinuity value
of a pixel against its neighbouring pixels~\cite{Gonzalez2002}. The
discontinuity can take on several types, for example colour, normal and depth. A square matrix,
called a kernel, is used to evaluate the discontinuity measure and is run
against every pixel in the image. Any pixels that fell below a specified
threshold hold are discarded, the remaining pixels are collected and aggregated
into higher level geometric primitives. In \threed space, features can be
classified under the broad categories of ridges, valleys and  silhouettes.
Ridges are formed by two front facing polygons with dihedral angle between 0 to
180, similarly, valleys are front facing polygons with dihedral angle between
180 to 360, both of these are exclusive, as an angle of 0 or 180 results in a
flat surface. Lastly silhouette edges are formed by the shared edge of front-facing
polygon and back-facing polygon. Raskar~\cite{Raskar2001} computes these \threed
features without connectivity information by extruding additional faces per polygon, using
existing geometries to hide the generated features if they turn out to be
inappropriate. In addition, since the features are geometric quads, additional processing can be
performed on these quads such that they can be rendered in a multitude of ways.
Hermosilla~\cite{Hermosilla2009} took this idea further by moving the extrusion
process to modern graphic pipeline's geometry shader, removing the need to do
the computations on the CPU. It should be noted that in interactive environment,
geometric features are generally view dependent in both \threed and \twod
scenarios, this usually means that feature extraction operation need to run on a
per frame basis.
 

When it comes to expressiveness, NPR rendering styles can be used to convey a
variety of semantics. Of particular interest to us is the idea of uncertainty:
graphics that are visible to the viewer, but also convey a sense of error
and inaccuracy. Researches in ancient architectural
reconstructions found that straight, solid
strokes convey higher degree of certainty than strokes that are drawn faded and
curved~\cite{Masuch1998, Strothotte1999}. The relevance here to our work is
that we use this type of technique to covey objects that are not in the focused context.


    % === Figure === 
	\begin{figure}
	 \centering  
	 %\includegraphics[width=\columnwidth]{copyright/viola.png}  
	 \includegraphics[scale=1.0]{copyright/viola.png}  
	 \caption[Importance-Driven Volume Rendering]{Importance-Driven Volume
	 Rendering~\cite{Viola2004}.
	 \copyright 2004, IEEE. Reprinted with permission.}
	 \label{figure:viola}
	\end{figure} 
	% ==============


Another branch of non-photorealistic rendering deals with semantic segmentation
of objects in the scene. For example illustrations found in science and
medical literature where specific parts are accentuated using different colours.
This is done in order to differentiate normal and outlier regions in the final
illustration. These type of NPR techniques deals with visibility issues, to
ensure that semantically important objects are easily visible and to provide features to deal with occlusion. 
One prominent research work came from an importance-driven function that can
be used to determine the visibility of objects~\cite{Viola2004} (See Figure
\ref{figure:viola}). In this paper scores are assigned to volumetric components such that more important
objects are rendered more opaque than objects of lesser importance. A screen
door transparency technique is introduced to ensure overall visibility by
rendering holes into the outermost volumetric layers. The idea of
segmentation and context is taken further by Tietjen \etal, they described a
segmentation scheme that partitions different volumes into focus, near focus and
context categories~\cite{Tietjen2005}, with the following semantics:
\begin{itemize}[noitemsep]
  \item Focus object: Current focus and is emphasized.
  \item Near focus object: Object for understanding spatial location or
  interrelations.
  \item Context Object: All other objects in the scene.
\end{itemize}
The segmentation allows for further accentuation of specific regions, while
still providing enough background context to show the subject matter.

%In action, focus, near-focus and context objects can be interactively changed
%and take on different artistic styles.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Integrated Visualization}
Traditionally Information Visualization deals with abstract
representation of data, such as structural or context visualization of text
data. Scientific Visualization, on the other hand, deals with visualization of concrete subjects such as physical 
or anatomical objects. However, while these are two distinct communities, the 
division is not always obvious. Many applications combine elements and techniques 
from both fields, though the two sides remain separate disciplines. As data 
becomes more heterogeneous, new approaches for data visualization make the case 
of whether the boundary should exist or not, as illustrated by recent attention 
to discuss the similarities and differences between the two groups~\cite{Hauser2005, Weiskopf2004}. 
The SciVis community in particular has
adopted techniques and practices that are traditionally in the InfoVis community, 
a well-known example is the volumetric visualization from Doleisch \etal, where 
multiple coordinated views, and linking and brushing techniques are used to 
further help the exploration process~\cite{Doleisch2003}.

    % === Figure === 
	\begin{figure}
	 \centering  
	 \includegraphics[width=\columnwidth]{copyright/sedlmair.png}  
	 \caption[CarComVis]{User interface for CarComVis~\cite{Sedlmair2009}.
	 \copyright 2009, Springer. Reprinted with permission.}
	 \label{figure:seldmair}
	\end{figure} 
	% ==============

For a more hybrid space approach between InfoVis and SciVis,  Balabanian \etal
placed hierarchical renderings of human anatomy inside an interactive
balloon-tree graph \cite{Balabanian2010}. In this graph, each anatomical object occupies a node, and is arranged by
logical hierarchy. The colours, size and arrangement of the nodes are dictated by abstract 
semantics, such as what is currently selected, filtering operations, 
and relations among the anatomical parts. The \threed rendering supports SciVis
operations such as viewing, slicing and picking of objects. Changes are linked
and propagated across relevant nodes and volume renders.


In the InfoVis space, researches done by Sedlmair \etal looked at enriching
InfoVis visualizations with \threed models~\cite{Sedlmair2009}.
In this work, they presented two applications, CarComVis which allows users to
explore intercommunication among vehicle components, and LibVis which explores
environmental reading in a library setting. Neither one is explicitly about
physical objects, but contains inherent spatial information for reconstructing a
\threed representation: An automobile model for CarComVis and a virtual library
environment for LIbVis. Sedlmair \etal explores different types of integration
techniques, including the usage of multiple coordinated views and a total immersion 
environment where the user is in a virtual world. Subjective feedback 
from their study suggested a strong desire for integration of \threed
visualizations into traditional visualization and work flow. Our work in this
thesis is perhaps mostly closely associated with CarComVis (see Figure
\ref{figure:sedlmair}), however our renderings are driven by unstructured text,
and our interactions are directly applicable to the visualization itself rather than through multiple coordinated
views.

%\textbf{Summary:} We take an approach similar to CarComVis [Seldmair2009], we
%generated a virtual vehicle based on the components mentioned in our dataset. However, rather 
%than using a coordinated views for information seeking, our interactions are more 
%tightly coupled with the \threed visualization. Also, our visualization style is
%based on the context of the dataset, rather than using a fixed visualization.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Focus+Context Techniques}
Focus+Context is a well-known and practiced technique in InfoVis, it is used to
draw attention to particular areas of interest while maintaining a certain degree of contextual 
information. There are various ways to discriminate these areas, for example 
Hauser~\cite{Hauser2006} reviewed the general approaches where usage of colour, 
spatial-location, opacity and other resources are demonstrated. One interesting 
use of the focus+context technique is the idea of using lenses, the lens traces 
back to a real world metaphor of a magnification lens, where the area directly 
under the lens are distorted and enlarged by the optics. In interactive
visualization, the lens acts as a specialised user interface widget, where the
data graphics under the lens undergoes a graphical transformation in order to
represent different semantics.


Fisheye Lens~\cite{Sarkar1992} is a technique for enlarging
details on a \twod image, for example on digital maps and circuit diagrams. Much
like its real world counterpart, a fisheye lens technique creates an ultra-wide viewing perspective that
creates a distortion displacement under the lens. The amount of displacement 
is controlled via a drop-off function that determines the degree of enlargement 
and shrinkage, typically with the central part of the lens enlarged while area
regions near the circumferences shrinks to compensate. There are a
few usability issues with fisheye lens, and likely the same issue applies to all
lenses that undergoes geometric distortion: it is difficult to tell where the
focus is, because the lens itself may occludes nearby objects which the viewer
use as navigational cues.

Although not directly a focus+context scenario, the original idea of using a
flexible and multipurpose virtual lens likely came from research by Bier \etal,
where a transparent overlay can be used to modify the appearance and behaviour
of objects underneath the overlay panel \cite{Bier1993}. Perhaps more
significantly, the lens acts as an interactive layer, where actions can be send
through the lens directly to the objects beneath it.


NPRLens~\cite{Neumann2007a} follows up on the MagicLens metaphor from Bier
\etal, allowing users to interactively create images in a non-photorealistic
style. NPRLens performs image processing on the \twod projections of objects in
\threed space. Graphical data points are first projected to screen space,
collected and aggregated into higher level graphical constructs such as line
segment and curve primitives. These can then be further manipulated such as
stretching, shrinking or changing the stroke styles of each primitive unit.


\threed MagicLens~\cite{Viega1996} is likely the first foray of the lens idea
taken into 3 dimensional space, unlike the previously mentioned works, MagicLens works with 
viewing volumes rather than \twod image maps. The lens in this scenario slices
the scene into different frustum volumes, each volume is then rendered separately 
and then recomposed together to create the final scene. This technique allows 
people to create interesting effects, for example exposing the skeletal 
structures of multi-layered objects. With more advanced hardware features, it is
possible to buffer the rendering into textures and combine them at a later
stage, rather than physically cutting the scene in volumes, this is the
technique that we use in our work.


Other than stylization and distortion techniques, the same lens metaphor can
also be used as a way to augment information seeking, in this context the lens 
is an exploration device that exposes hidden data points, or data points that 
cannot be feasibly displayed in a readable manner. Generally these cases arise
because of overlapping points, or there are too many points on the screen,
making labelling of all points impractical as they introduce too much visual
clutter. Excentric Labeling~\cite{Fekete1999} uses the lens metaphor to deal
with densely populated data points on a \twod illustration. Data points are not
labelled, instead, when the lens is hovered over the locations that contain data
points, line segments are extended from the data points outward towards the
circumference of the lens. The actual text labels and descriptions are drawn
along the circumference, allowing viewers to make the connection between
graphics and text. The lens itself is movable which allows for exploration of
interesting regions. Extended Excentric Labeling~\cite{Bertini2009} (Figure
\ref{figure:bertini}) further extends on the idea by creating additional
interactions with the lens, dynamic overviews are shown inside the lens as miniature graphs summarizing of objects under the lens. The
labels are scrollable to allow exploration of dense areas without overcrowding
screen space, the layout of labels and the extending lines are also altered to
minimize crossings which can cause additional visual complexity. 
 
    % === Figure === 
	\begin{figure}
	 \centering  
	 \includegraphics[width=\columnwidth]{copyright/bertini.png}  
	 \caption[Extended Excentric Labelling]{Extended Excentric
	 Labelling~\cite{Bertini2009}.
	 \copyright 2009, The Eurographics Association and Blackwell Publishing.
	 Reprinted with permission.}
	 \label{figure:bertini}
	\end{figure} 
	% ==============


%While both Excentric and Extended Excentric Lens operate in \twod space, there
%are some researches with that takes the same ideas into environments that are
%composed of \threed objects. 

While the techniques above are largely applied to \twod applications, there are
applicable \threed focux+context techniques. Sonnet \etal \cite{Sonnet2004}
created a medical volume rendering system where the lens takes the form of a \threed cursor,
although viewers cannot see into the cursor itself, the cursor creates a
spherical volume that pushes other volumetric objects away from the center, thus
creating more space for labels and annotations, and reduce the occlusion issues that
come naturally with \threed environment. Another system is BrainGazer
\cite{Bruckner2009}, a volumetric visualization application that allows
users to draw a query path onto a segmented volume data. The path exhibits lens
like behaviour, that is, the systems queries the objects that are in
proximity to the path drawn. Additional information about these objects is then
shown in a separate user interface panel. Our system uses a similar approach, in
that the scene we render is pre-segmented into regions of different
semantics, though we use an actual lens as the querying tool, and we deal with
geometric rather than volumetric data.

 
%\textbf{Summary:} Our approach to focus+context resembles a combination of
%approaches taken by BrainGazer, NPRLens and Excentric Labels. Like BrainGazer, we allow 
%people to form visual queries via indexing spatial location with different semantics, 
%though we take a proactive approach to highlight and link the text data back to 
%the visualization by combining ideas from NPRLens and Excentric Labels.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Design for Touch Interface}
As the cost of touch surface computing becomes cheaper, we are gradually seeing 
interactive surfaces introduced into public areas, office work environments and 
personal spaces. Interactive interfaces brings new possibilities of exchanging 
information in a walk-by scenario \cite{Vogel2004}, allowing people to
interact with data without the need of peripheral hardware devices such as the
mouse or keyboard. Instead, people utilize their hands and fingers as
interactive mediums.

Though much of the work today deals with either collaborative or
distributed environments, the focus in this work deals with designing
surface-based gestures for a large display space. Gesture design
and classification have been looked at via crowd sourcing approaches,
leveraging public opinions to come to a consensus as to which action should be
associated with what gestures. For example, Wobbrock \etal conducted experiments
where the consequences are given, and participants had to come up with their own appropriate gestures to cause the said
actions~\cite{Wobbrock2009}. Others, such as Hinrichs and Carpendale, looked at
how environmental and social context affect how people perform gestures
~\cite{Hinrichs2011}. In both cases, experimental results converge and they were
able to create high-level classifications. On the other hand, these
classifications are derived from independent, low-level tasks, where our
prototype looks at a specific problem from end-to-end. As such, these
classification servers as inspirations rather than dictating our gesture design.

Aside from classification, there are many other nuances and device specific
attributes in gesture design that one needs to pay attention to make them
more user friendly. Providing appropriate visual feedback, dealing with
false-positive/false-negative touches, and target acquisition are some of
important details for natural interactions. Much of the intricacies are outline
by Wigdor and Wixon, where they discussed design principles and other
guidelines~\cite{Wigdor2011}.


 

