%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Related Works
% - Show a picture of Wordle (A section of the thesis maybe)
% - Show a picture of WordsEye
% - Probably want to quote Carpendale's work on Elastic Framework
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Related Work}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Fundamentals}
Visualization revolves around the process of of mapping and representing data or
concept graphically such that they are visible to the human eye. Visualization, 
in theory,  takes advantage of the human perceptual system to rapidly understand
what the data is communicating to the readers. Preattentive processing, 
for example, deals with the extraction of graphical features without the need to
consciously focus one’s attention to the details, and it can be achieved extremely 
fast and in parallel (Ware?), and thus provide an important concept for drawing 
out the salient features in graphics. To communicate clearly, it is crucial to 
select the appropriate visual representation, termed visual variables (Bertin). 
Visual variables dictates how a graphical mark is drawn and includes the mark’s 
position, size, shape, value, colour, orientation and texture. Each visual 
variable can then be evaluated for different characteristics, such as selective, 
associative, quantitative, order and length. We summarize visual variables 
in <table> below. With the advent of computer technology, the idea of motion or 
animation as another visual variable was also proposed. (Carpendale). 

Another important player in how human perceive graphics is based on the field of
psychology, with the concept of Gestalt Perception. The fundamentals of Gestalt 
perception deals with the likelihood that human prefers to see things as a whole, 
rather than individual parts. This in turn influences our comprehension and how 
we choose to see interesting patterns because of our tendencies to 
group/differentiate objects by proximity, similarity and closure.

In order to be useful, any visualization system should support user tasks that
allow rich and meaningful exploration of data. Shneiderman’s (Shneiderman1996)  
information seeking mantra set the standard for exploratory graphical user 
interface. They are, in order:
\begin{itemize}
  \item Overview: See and summarize the entire collection
  \item Zoom: Zoom in on items of interest
  \item Filter: Removal of uninteresting items
  \item Detail-on-Demand: Select item/group and get details when needed
  \item Relate: View how an item relates to others
  \item History: Keep a history of actions to support undos and redos
  \item Extract: Allow extraction of subsections
\end{itemize}
While not all visualizations will support all seven tasks, they provide a good
starting point for designing visualization applications.

\textbf{Summary:} In our system, we are mostly concerned with the exploration of
data, taking advantage of people’s ability to see colour and proximal objects. We follow 
Shneiderman’s information seeking mantra in spirit: we have no support for 
history or extraction functions, though these could be interesting directions 
for our future work. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Descriptive Rendering}
Descriptive illustration can be considered to be a sub field of computer
graphics, which focuses on static illustrations or an animated sequence based on
text input. It can be used as an analytic tool by literally rendering the text
or render an interpreted representation of the text. Unlike other text analytics
tools or text visualizations that presents an abstract visuals, descriptive
visualization is highly concerned with understanding of natural language to
render a scene that is a literal representation of the underlying text. 

Though strictly speaking not a text-to-scene application, the IBIS application
from Seligmann and Feiner renders a \threed scene based on a set of user
provided rules (Seligmann1991). These rules specify which object and locations
in the \threed scene have higher degree of interest. The application itself
evaluates these rules and determines the optimal viewing perspective, as well
the application adjust the lighting conditions as a way to draw attention to
specific details in the scene. 

One of the most fully-realized text-to-scene generation system is
WordsEye\cite{Coyne2001}. WordsEye uses a combination of grammatical rules 
and heuristics to determine subjects and their actions. The subjects are mapped 
against a large repository of \threed models, and the verb actions are mapped to 
predefined poses. One of the interesting WordsEye strives for is the preservation 
of spatial relations among subjects in the text, for example: “The giant orange 
cat is on the black chair.” will literally rendering a large cat on top of a chair. 
However, WordsEye is still bound by the inherent ambiguities in written language 
and the accuracy of its semantic interpretation. As far as the rendering process 
is concerned, WordEye can only render objects if the model is available in its 
repository.

Another example of text-to-scene generation is CarSim\cite{Akerberg2003},
which uses a similar logical processing and rendering process, but is specifically designed
to deal with the re-creation of car accidents by parsing traffic accident reports. 
Where CarSim differs is that it tries to generate an animated sequence by 
inferring the speed and direction of vehicles from the text content, it is less 
concerned with the literal representation of the car, but the situation 
surrounding the incident.

Interpretive examples includes Wordle[???] which looks at representing document
content by counting word occurrences. Unlike those mentioned above, Worlde does 
not perform any semantic inferences on the text, but rather left the graphic 
generation up to the users, allowing them tight fit the words into contour shapes, 
that often represents the underlying themes. For example; a bibliographical text 
about a person may use that person’s silhouette as the fitting contour. Wordle’s
layout algorithm disallows overlaps, and allows users to directly interact with
the text visualization. Another example is calligraphic packing \cite{Xu2007},
which takes a more artistic approach. This research visualizes single words by 
distorting individual glyphs to fit into \twod regions. Creative freedom is
given to the people to choose which image best represents the word. However, due to 
concentration on artistic style, it is questionable whether individual letters, 
and the whole word is still recognizable after the transformation.

\textbf{Summary:} Unlike WordsEye and CarSim, we start with a scene that is
pre-constructed, that is, we know ahead of time the subjects in our text 
documents and can place them into \threed space. While it is interesting to use 
grammatical approach to grab context, we found it more generalizable to use a 
dictionary approach similar to Wordle for parsing text because we iterate 
over thousands of documents.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Non-Photorealistic Rendering}
Non-Photorealistic Rendering can be described as any computer generated images 
that do not involve the accurate simulation of light. While photorealistic images 
presents accurate details, often times it is not entirely necessary to present 
such high degree of accuracy. On the contrary, specular reflections, shadows, 
occlusions and other natural lighting phenomenons can hinder the ability to view 
vital details and create ambiguities, which distorts the ability of the viewer 
to accurately interpret the image.

This lack of realism allows a greater degree of freedom in how the final rendering 
can look and what message it conveys, which is often done with an artistic flare, 
for example, the simulation of blueprints and other hand-drawn artworks. 
Non-photorealistic lighting from Gooch \etal \cite{Gooch1995}, looks at
altering traditional Phong-shading equations to resemble technical drawing found in textbooks. 
Their lighting equation removes the specular component, instead a flat shading 
style is used to transition between cool-to-warm tones, preserving surface 
orientation and edge details.

Another major feature of NPR is that we can use it to simplify and accentuate 
object geometries, taking advantage of visual perception capabilities of seeing 
continuities and surfaces (Site Ware?), we can reduce the visual clutters of an 
image or a scene without compromising the important details. This is known as 
feature extraction and can be done in \twod or \threed space. The \twod space
algorithm looks at discontinuities of a single pixel against its neighbouring pixels [Cite Book]. 
There are various discontinuity measures, for example colour discontinuity, depth 
discontinuity and normal discontinuity. These are often combined to improve the 
detection accuracy. If the current pixel value difference is high enough then the 
two pixels are considered to be from different objects and the pixel itself is a 
feature because it marks the object boundary. This algorithm iterates through every 
pixel in the image and collects all pixels that meet the feature criteria, further 
processing can then be performed to form line-segments from the selected pixels. 
In \threed space, the discontinuity takes form of ridges and valleys formed by
adjacent polygons. Ridges are formed by two front-facing polygons with an acute angles, 
while valleys are created by obtuse angles. Raskar \cite{Raskar2001} uses
hardware pipeline to handle the discovery of ridges and valleys, at each rendering frame the front 
and back facing polygons are are computed and additional quad primitives are extruded 
for both valleys and ridges. Because these quads are computed and stored, they can 
take on additional stylistic properties. Hermosilla \cite{Hermosilla2009} took
this idea further by doing this directly on the hardware pipeline, using hardware’s geometry 
shader to create primitives instead of doing computation on the CPU. Note because 
features tend to be view-dependent, most algorithms are recalculated on a per-frame basis.

When it comes to expressiveness, NPR rendering styles can be used to convey a
variety of styles. Of particular interest to us is the idea of uncertainty:
graphics that are visible to the viewer, but also visible is a degree of error
or inaccuracy. Works done in this area includes researches from Strothotte, Masuch 
and Isenberg, they looked at using lines and stroke primitives
to reconstruct ancient architectures \cite{Strothotte1999, Masuch1998}. 
As much of the buildings left standing today are in ruins, the renderings are 
meant to show what the buildings may have looked like in the past. They found
that straight, solid strokes conveys a higher degree of certainty than strokes
that are drawn faded and sketch like. While this result isn't overly surprising
(It matches the types of drawings done by artists), together with
different style of pen-and-ink renderings (Salesin) they created a automated way
to imitate the type of sketches done by hand.

Another branch of non-photorealistic rendering deals with semantic segmentation
of objects in the scene, such as illustrations found in science and medical 
literature where different parts are highlighted in different colours in order 
to differentiate importance. These type of techniques looks at visibility issues, 
to ensure that semantically important objects are easily visible and to provide 
features to deal with occlusion issues. One prominent research came from an 
importance-driven function that can be used to determine the visibility of 
objects \cite{Viola2004}. In this paper the authors assign scores to volumetric 
components such that more important objects are rendered more opaque than objects 
of lesser importance. The authors also introduce a screen-door transparency 
technique to ensure overall visibility and context by rendering “holes” into 
the outer layers. This idea of segmentation and context is taken further in Tietjen’s 
paper on volumetric medical visualizations where different sub volumes are 
divided into focus, near-focus and context components \cite{Tietjen2005}. In
this scheme, different rendering styles are used for focus, near-focus and context objects to 
maximize visibility without losing overall context of what the image is about. 
In addition, contour lines are introduced to further enhance object recognition 
abilities. 

\textbf{Summary:} While there are multitude of other NPR related works, we have
chosen to focus on techniques that accentuate geometric features without 
sacrificing realism. The advantage of most of these techniques is that they are 
parameterizable, and can be combined to create composite effects.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Integrated Visualization}
Traditionally Information Visualization deals with abstract dataset, such as 
structural or context visualization of text data. Scientific Visualization, on 
the other hand, deals with visualization of concrete subjects such as physical 
or anatomical objects. However, while these are two distinct communities, the 
division is not always obvious. Many applications combine elements and techniques 
from both fields, though the two sides remain separate disciplines. As data 
becomes more heterogeneous, new approaches for data visualization make the case 
of whether the boundary should exist or not, as illustrated by recent attention 
to discuss the similarities and differences between the two groups
\cite{Hauser2005, Weiskopf2004}. The SciVis community is in particular has
adopted techniques and practices that are traditionally in the InfoVis community, 
a well known example is the volumetric visualization from Doleisch \etal, where 
multiple coordinated views, and linking and brushing techniques are used to 
further help the exploration process \cite{Doleisch2003}.

In a more hybrid space between InfoVis and SciVis,  Balabanian \etal placed
hierarchical renderings of human anatomy inside an interactive Balloon Tree? graph. 
In this scheme, each anatomical object occupies a node, and is arranged by logical
hierarchy. The colours, size and arrangement of the nodes are dictated by abstract 
semantics, such as what is currently selected, filtering operations, 
and relations among the parts. The \threed rendering supports SciVis operations
such as viewing, slicing and picking objects. Changes are linked and propagated across 
relevant nodes and volume renders. 

In the InfoVis space, researches done by Sedlmair \etal looked at enriching
InfoVis with \threed model representations (Sedlmair2009). In this work, they
presented two applications, CarComVis which looks at intercommunication among vehicle 
components, and LibVis which explores environment settings in a library. Neither 
dataset is explicitly about physical objects, but contains inherent spatial 
information for constructing a \threed representation. (An automobile for
CarComVis and a virtual library for LIbVis). Sedlmair \etal explores different
types of integration techniques, including the usage of multiple coordinated views and a 
total immersion environment where the user is in a virtual world. Subject feedback 
from their study suggest a strong desire for integration of \threed
visualizations.

\textbf{Summary:} We take an approach similar to CarComVis [Seldmair2009], we
generated a virtual vehicle based on the components mentioned in our dataset. However, rather 
than using a coordinated views for information seeking, our interactions are more 
tightly coupled with the \threed visualization. Also, our visualization style is
based on the context of the dataset, rather than using a fixed visualization.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Focus+Context Techniques}
Focus+Context is a well known and practiced technique used to draw attention to
particular areas of interest while maintaining a certain degree of contextual 
information. There are various ways discriminate these areas, for example 
Hauser\cite{Hasuser2006} reviewed the general approaches where usage of colour, 
spatial-location, opacity and other resources are demonstrated. One interesting 
use of the focus+context technique is the idea of using lenses, the lens traces 
back to a real world metaphor of a magnification lens, where the area directly 
under the lens are distorted and enlarged by the optics. Translating this into the 
graphics realm, the lens acts as a specialised user interface widget, where the 
data graphics under the lens widget undergoes a some type of transformation to 
represent different semantics. 

Fisheye Lens\cite{Sarkar1992} is a well known lens technique for enlarging
details on a \twod image, for example on digital maps and circuit diagrams. Much
like its real world counterpart, a fisheye lens technique creates an ultra wide viewing perspective that
creates a distortion displacement under the lens. The amount of displacement 
is controlled via a drop-off function that determines the degree of enlargement 
and shrinkage. For example a Gaussian like function is used to enlarge the 
center area of the lens while shrinking the area near the lens circumference, 
drawing readers’ attention to the enlarged portion.

NPRLens\cite{Neumann2007a} performs image processing on the \twod projections of
objects in \threed space. Graphical data points are first projected to screen
space, collected and aggregated into higher level graphical constructs such as line 
segments and curves. These can then be further manipulated such as stretching, 
shrinking or changing the stroke styles of each line segments. Note performing 
such actions directly on the \threed objects can create tears in the object
model meshes.

MagicLens\cite{Viega1996} is likely the first foray of the lens idea taken into
3 dimensional space, unlike the previously mentioned work, MagicLens works with 
viewing volumes rather than \twod image maps. The lens in this scenario slices
the scene into different frustum volumes, each volume is then rendered separately 
and then recomposed together to create the final scene. This technique allows 
people to create interesting effects, for example exposing the skeletal 
structures of layered objects.

Other than stylization and distortion techniques, the lens lens metaphor can
also be used as a way to augment information seeking, in this context the lens 
is an exploration device that exposes hidden data points, or data points that 
cannot be displayed in a readable manner. Generally these cases arise because 
of overlapping points, or too many points on the screen, making labelling of 
all points impractical. Excentric Labeling \cite{Fekete1999} uses the lens
metaphor to deal with densely populated data points on a \twod drawing. Data
points are not labelled, instead, when the lens is over the locations of the data points, 
line segments are extended from the data points outward towards the circumference 
of the lens. The actual text labels and descriptions are drawn along the 
circumference, allowing people to make the connection between graphics and text. 
The lens itself is movable and allows for exploration of interesting areas. 
Extended Excentric Labeling \cite{Bertini2009} further extends on the idea by
creating additional interactions with the lens, dynamic overviews are shown inside the lens 
miniature graphs composed of objects under the lens. The labels are scrollable to 
allow exploration of dense areas without overcrowding screen space, the labels and 
the extending lines are also altered to minimize crossings which can cause 
additional visual complexity. As for labelling placement, Ali et. al in their 
paper described and categorize general layout types \cite{Ali2005}, they
introduce a way to calculate labels’ end-points in image space using an indexed colouring scheme.

While both Excentric and Extended Excetric Lens operate in \twod space, there
are some researches with the same idea in \threed space. Sonnet \etal
\cite{Sonnet2004} created a medical volume rendering system where the lens takes
the form of a \threed cursor, the cursor creates a spherical volume that pushes
other volumes away from the center, thus creating more space for labels and annotations. BrainGazer
\cite{Bruckner2009} allows users to draw path a query onto a segmented volume
rendering. The path behaves as a visual query, where semantic data of segments  
near the path are collected and displayed in a separate pop-up panel.

\textbf{Summary:} Our approach to focus+context resembles a combination of
approaches taken by BrainGazer, NPRLens and Excentric Labels. Like BrainGazer, we allow 
people to form visual queries via indexing spatial location with different semantics, 
though we take a proactive approach to highlight and link the text data back to 
the visualization by combining ideas from NPRLens and Excentric Labels.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Design for Touch Interface}
As the cost of touch surface computing becomes cheaper, we have gradually seeing 
interactive surfaces introduced into public areas, office work environments and 
personal spaces. Interactive interfaces brings new possibilities of exchanging 
information in a walk-by scenario [Vogel2004], without the need of
peripheral hardware devices such as the mouse or keyboard. New challenges also arise in how 
best to interact with the touch surface. Wigdor and Wixon in their book touched 
upon the idea of natural user interfaces, in the sense that an interface design 
that feels intuitive to use rather than an organic process\cite{Wigdor2011}.
Other researchers looked into building touch interactions from a user-defined perspective 
by studying how people create their own gestures given a effect-cause scenario (Wobbrock2009), 
as well how gestures relate to different social context (Hinrichs2011).


\textbf{Summary:} While our primary focus is about the visualization of entities
in text.
We imagine a walk-up-and-use scenario where a single person can use the large 
screen display for impromptu exploration, or to use the display as presentation 
aid for meetings. Much of our design has been guided by past classifications of 
gestures like ones mentioned above, as well to keep our interactions quite simple 
to discourage any complexities that might arise from a social setting (Brignull2003).

