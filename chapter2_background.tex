%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Related Works
% * Expand Touch Surface subsection
% - Show a picture of Wordle (A section of the thesis maybe)
% - Show a picture of WordsEye
% - Probably want to quote Carpendale's work on Elastic Framework
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Related Work}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Fundamentals}
Visualization revolves around the process of of mapping and representing data or
concept graphically such that they are visible to the human eye. Visualization, 
in theory,  takes advantage of the human perceptual system to rapidly understand
what the data is communicating to the readers. 

Foremost we want to recognize that human visual system is quite limited, we
can only perceive a small fraction of the entire spectrum of light, and of our
field of vision, only a small area is truly in focus. However, there is a small
set of visual properties that can be detected extremely rapidly (comparatively
to other sensory systems) and accurately by low-level perceptual system known as
pre-attentive processing \cite{WAR2004b}. In theory, this perceptual feature
deals with the extractions of specific graphical features without the need to 
consciously focus attention on the details, allowing viewers to spot salient and 
outlier features in a single glimpse. For example, picking out a red circle 
out of a group of blue circles is considered instantaneous, barring any visual 
abnormalities. In terms of design, exploitation of pre-attentive processing can 
greatly increase the rate of comprehension and ease of use.

To establish clear visual communication, it is crucial to have a set of visual
building blocks that can be grouped and arranged to convey information. Much of
the initial work came from the field of cartography, particularly with the ideas
of marks and visual variables from Bertin \cite{BER1983a}. Bertin defined seven
variables, including position, size, shape, value, colour, orientation and
texture. Each visual variable can have certain characteristics that can be used
to determine which of the seven visual variables is most appropriate visual
representation. These characteristics includes properties such as selective,
associative, quantitative, order and length. A summary table below outlines the
relationship between each variable and its attributes.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
 & Position & Size & Shape & Value & Colour & Orientation & Texture  \\  
\hline
\hline 
Selective &  Yes & Yes & Maybe & Yes & Yes & Yes & Yes \\ 
\hline
Associative &  Yes & Yes & Maybe & Yes & Yes & Yes & Yes \\
\hline
Quantitative &  Yes & Maybe & No & No & No & No & No \\
\hline 
Order &  Yes & Yes & No & Yes & No & No & No \\
\hline
Length & Yes & Yes & Maybe & Yes & Yes & Yes & Yes \\
\hline
\end{tabular}\caption{List of visual variables and
their characteristics}\label{table:VisualVariable}
\end{table}

%With the advent of computer technology, the idea of motion or 
%animation as another visual variable was also proposed. (Carpendale). 


%Another important factor in how human perceive graphics is based on the field
% of psychology, with the concept of Gestalt Perception. The fundamentals of Gestalt
%perception deals with the likelihood that human prefers to see things as a whole
%rather than individual parts \cite{WAR2004b}.

%This in turn influences our
%comprehension and how we choose to see interesting patterns because of our tendencies to 
%group/differentiate objects by proximity, similarity and closure.

In terms of building a visualization system, there are several key contributions
in this area. The visualization pipeline from Card \etal (Card) describes a
step-by-step process of data manipulation and mapping as follows:
\begin{itemize}[noitemsep]
  \item Analysis: Normalization of raw data for visualization
  \item Filter: Select subsections of data to be visualized
  \item Mapping: Find the appropriate visual representations
  \item Rendering: Transform raw data into image data
\end{itemize}
For an interactive visualization, these stages are not a linear, one-off
process. The stages of Filter, Mapping and Renderings form a repetitive loop as
the users change parameters to explore different parts of the visualization. It
is often through exploration that enables users to find and extract interesting
data. The process of facilitating useful graphical-based exploration is
advocated in Shneiderman's information seeking mantra \cite{Shneiderman1996} and
can be broken down into these specific tasks:
\begin{itemize}[noitemsep]
  \item Overview: See and summarize the entire collection
  \item Zoom: Zoom in on items of interest
  \item Filter: Removal of uninteresting items
  \item Detail-on-Demand: Select item/group and get details when needed
  \item Relate: View how an item relates to others
  \item History: Keep a history of actions to support undo and redo
  \item Extract: Allow extraction of subsections
\end{itemize}
Each task can then be further broken down by different types of interaction
techniques. Careful consideration should also go into each task to determine if
it is actually appropriate for the data and problem at hand.



%\textbf{Summary:} In our system, we are mostly concerned with the exploration
% of data, taking advantage of people’s ability to see colour and proximal objects. We follow 
%Shneiderman’s information seeking mantra in spirit: we have no support for 
%history or extraction functions, though these could be interesting directions 
%for our future work. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Descriptive Rendering}
Descriptive illustration can be considered to be a sub field of computer
graphics, which focuses on static illustrations or an animated sequence of
images based on text input. It can be considered as a text-analytic tool because
it renders the text in a literal sense, or through some kind of interpretive
representation. Systems that attempts to do literal renderings are concerned
with understanding of natural languages, whereas interpreted rendering systems
tend to some basis intentions that are user based.


Though strictly speaking not a text-to-scene application, the IBIS application
from Seligmann and Feiner renders a \threed scene based on a set of user
provided rules \cite{Selgmann1991}. These rules specify which
object and locations in the \threed scene have higher degree of interest. The application itself
evaluates these rules and determines the optimal viewing perspective, as well
the application adjust the lighting conditions as a way to draw attention to
specific details in the scene. To our knowledge this is one of the first system
where a pre-constructed scene can be modified by text input.

One of the most fully-realized text-to-scene generation application is
WordsEye\cite{Coyne2001}. WordsEye uses a combination of grammatical rules 
and heuristics to determine subjects and their actions. The subjects are mapped 
against a large repository of \threed models, and the verb actions are mapped to 
predefined poses. One of the interesting WordsEye strives for is the preservation 
of spatial relations among subjects in the text, for example: ``The giant orange 
cat is on the black chair.'' will literally rendering a large cat on top of a chair. 
However, WordsEye is limited by the inherent ambiguities in written language and
the accuracy of its semantic interpretation. 

Another example of text-to-scene generation is CarSim\cite{Akerberg2003},
which uses a similar logical processing and rendering process, but is specifically designed
to deal with the re-creation of car accidents by parsing traffic accident reports. 
Where CarSim differs is that it tries to generate an animated sequence by 
inferring the speed and direction of vehicles from the text content, it is less 
concerned with the literal representation of the car, but with the situation 
surrounding the incident.


%So far, the discussion have been around examples where the renderings are based
%on the literal meaning of the text. On the other side are interpretive
%renderings, where the images produced are not necessarily the ground-truth of
%the text, but often produced with artistic flares and particular theme in mind.

On the interpretive side, a popular example is the web-based application called
Wordle(???). Unlike the systems mentioned above, Wordle does not perform any
semantic inferences on the underlying text, it rather collects frequency
statistics of each string tokens. Unlike the closely related tagcloud, Wordle
allows people to fit the most frequently occurring words into arbitrary \twod
geometric shapes. This give creativity control over to the users, which in
practice lead to the usage of symbolic shapes that exposes underlying themes in
the text. For example, a Wordle visualization of a text document about the
economy will use a dollar-sign shape to instill the idea that the visualization
is related to monetary issues.

Another example is calligraphic packing \cite{Xu2007}, which takes a more artistic approach. 
This research visualizes single words by distorting individual glyphs to fit into \twod regions. 
Creative freedom is given to the people to choose which image best represents
the word, as well as the amount of acceptable distortion. However, due to
concentration on artistic style, the legibility of the actual words after the
distortion transformation is not guaranteed.


%\textbf{Summary:} Unlike WordsEye and CarSim, we start with a scene that is
%pre-constructed, that is, we know ahead of time the subjects in our text 
%documents and can place them into \threed space. While it is interesting to use 
%grammatical approach to grab context, we found it more generalizable to use a 
%dictionary approach similar to Wordle for parsing text because we iterate 
%over thousands of documents.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Non-Photorealistic Rendering}
Since the infancy of computer graphics, researchers have long been looking at
ways to render photorealistic renderings, images that are nearly
indistinguishable than that of a photograph taken by a camera. In recent years,
however, another area of computer graphics research have been looking in the
opposite direction: Non-Photorealistic Rendering. Non-Photorealistic Rendering can be 
described as any computer generated graphics that do not involve the accurate 
simulation of the behaviour of light. While photorealism has long been the holy
grail in graphics research, there are compelling reasons for choosing NPR
representations. First, NPR images are judged based on their ability to
communicate a specific message to their viewers, thus they are driven to
emphasize features in a scene or some underlying attributes \cite{Gooch2001}.
This is in contrast with photorealistic rendering where they are compared
against a ground-truth image. Second, often times it is undesirable to preserve
a high degree of accuracy, specular reflections, shadows and other natural
lighting phenomenons can hinder viewers ability to see vital details and create
ambiguities. NPR can choose to ignore the natural laws of physics, and instead
choose to focus on different abstractions such that the illustration can be
meaningful to people from different fields. Because there are multitude of
possibilities in NYPR, this section solely focuses on prior works that related
to our work here, or served as our inspirations.


Images found in technically written materials, or that of instructional manuals
are quite different than those found in photo scrapbooks. The key point, as
stated above, is that communication is valued above realism. Early works by
Gooch \etal uncovered common themes in technical illustrations and ways to
automate some of these properties \cite{Gooch1998}. Gooch \etal modifies the
conventional shading algorithm to use a two-tone approach that shift from
warm-colour to cool-colour, in addition, they added edge lines and removed
shadows. Illustrations created in this fashion have major benefits over
photography. One, strong lighting effects under conventional shading model are
reduced, preserving the surface details under the light patches. Two, the
removal of shadow regions shows the hidden details that were not previous
visible. Other NPR shading techniques also exist, for example
cel-shading/toon-shading techniques (???). Generally speaking, cel and toon
shading map the intensity of light from a continuous function into
discrete values, creating distinct contours where the values change.
These type of shading algorithms mimics those illustrations found in
hand-drawn illustrations such as comics. Other works looks at the simulation of
physical materials, for example textures as halftones to simulate stylistic
artistic sketches \cite{Freudenberg2002}. However it is semi-automatic as some
human input is required for the right combination of textures, normal maps and 
other special effects.
 

Another major benefits of NPR is that it can be used to simplify and
accentuate object geometries. By taking advantage of visual perception capabilities of
seeing continuities and surfaces as dictated by Gestalt Principals
\cite{WAR2004b}, human can see and complete entire shapes rather with just a few
geometric primitives. Feature extraction is one way to achieve this goal.
Feature extraction can be done in either \twod or \threed spaces. The \twod
algorithm is image based, and based upon calculating the a discontinuity valuee
of a pixel against its neighbouring pixels (Cite Book). The discontinuity can
take on several types, for example colour, normal and depth. A square matrix,
called a kernel, is used to evaluate the discontinuity measure and is run
against every pixel in the image. Any pixels that fell below a specified
threshold hold are discarded, the remaining pixels are collected and aggregated
into higher level geometric primitives. In \threed space, features can
classified as ridges and valleys silhouettes. Ridges are formed by two front
facing polygons with dihedral angle between 0 to 180, similarly, valleys are
front facing polygons with dihedral angle between 180 to 360, both of these are
exclusive, as angle of 0 or 180 results in a flat surface. Lastly silhouette
edges are formed by the shared edge of front-facing polygon and back-facing
polygon. Raskar \cite{Raskar2001} computes these \threed features without
connectivity information by extruding features per polygon, using existing
geometries to hide the generated features there they are inappropriate. In
addition, since the features are geometric quads, additional processing can be
performed so they take on different styles. Hermosilla \cite{Hermosilla2009}
took this idea further by moving the extrusion process to modern graphic
pipeline's geometry shader, removing the need to do the computations on the CPU.
It should be noted that in interactive environment, features are view dependent
in both  \threed and \twod scenarios, this usually indicates that feature
extraction operation need to run once per frame.
 

When it comes to expressiveness, NPR rendering styles can be used to convey a
variety of styles. Of particular interest to us is the idea of uncertainty:
graphics that are visible to the viewer, but also visible is a degree of error
or inaccuracy. Works done in this area includes researches from Strothotte, Masuch 
and Isenberg, they looked at using lines and stroke primitives
to reconstruct ancient architectures \cite{Strothotte1999, Masuch1998}. 
As much of the buildings left standing today are in ruins, the renderings are 
meant to show what the buildings may have looked like in the past. They found
that straight, solid strokes conveys a higher degree of certainty than strokes
that are drawn faded and sketch like. While this result isn't overly surprising
(It matches the types of drawings done by artists), together with
different style of pen-and-ink renderings (Salesin) they created a automated way
to imitate the type of architectural sketches done by hand.

Another branch of non-photorealistic rendering deals with semantic segmentation
of objects in the scene, such as illustrations found in science and medical 
literature where different parts are highlighted in different colours. This is
done in order to differentiate normal and outlier areas. These type of
techniques look at visibility issues, to ensure that semantically important objects are easily visible and to provide 
features to deal with occlusion issues. One prominent research came from an 
importance-driven function that can be used to determine the visibility of 
objects \cite{Viola2004}. In this paper the authors assign scores to volumetric 
components such that more important objects are rendered more opaque than objects 
of lesser importance. The authors also introduce a screen-door transparency 
technique to ensure overall visibility and context by rendering ``holes'' into 
the outer layers. This idea of segmentation and context is taken further in
Tietjen \etal described a method for  volumetric medical visualizations
where different sub volumes are divided to take on different semantics \cite{Tietjen2005}. In summary
they divided any scene into three categories:
\begin{itemize}[noitemsep]
  \item Focus object: Current focus and is emphasized.
  \item Near focus object: Object for understanding spatial location or
  interrelations.
  \item Context Object: All others objects in the scene.
\end{itemize}
In action, focus, near-focus and context objects can be interactively changed
and take on different artistic styles.

%\textbf{Summary:} While there are multitude of other NPR related works, we have
%chosen to focus on techniques that accentuate geometric features without 
%sacrificing realism. The advantage of most of these techniques is that they are 
%parameterizable, and can be combined to create composite effects.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Integrated Visualization}
Traditionally Information Visualization deals with abstract
representation of data, such as structural or context visualization of text
data.
Scientific Visualization, on the other hand, deals with visualization of concrete subjects such as physical 
or anatomical objects. However, while these are two distinct communities, the 
division is not always obvious. Many applications combine elements and techniques 
from both fields, though the two sides remain separate disciplines. As data 
becomes more heterogeneous, new approaches for data visualization make the case 
of whether the boundary should exist or not, as illustrated by recent attention 
to discuss the similarities and differences between the two groups
\cite{Hauser2005, Weiskopf2004}. The SciVis community is in particular has
adopted techniques and practices that are traditionally in the InfoVis community, 
a well known example is the volumetric visualization from Doleisch \etal, where 
multiple coordinated views, and linking and brushing techniques are used to 
further help the exploration process \cite{Doleisch2003}.

In a more hybrid space between InfoVis and SciVis,  Balabanian \etal placed
hierarchical renderings of human anatomy inside an interactive balloon-tree graph.
In this graph, each anatomical object occupies a node, and is arranged by
logical hierarchy. The colours, size and arrangement of the nodes are dictated by abstract 
semantics, such as what is currently selected, filtering operations, 
and relations among the anatomical parts. The \threed rendering supports SciVis
operations such as viewing, slicing and picking objects. Changes are linked and propagated across 
relevant nodes and volume renders, integrating the SciVis and InfVis actions. 

In the InfoVis space, researches done by Sedlmair \etal looked at enriching
InfoVis with \threed model representations \cite{Sedlmair2009}.
In this work, they presented two applications, CarComVis which allows users to
explore intercommunication among vehicle components, and LibVis which explores
environmental reading in a library setting. Neither one is explicitly about
physical objects, but contains inherent spatial information for reconstructing a
\threed representation: An automobile model for CarComVis and a virtual library
environment for LIbVis. Sedlmair \etal explores different types of integration
techniques, including the usage of multiple coordinated views and a total immersion 
environment where the user is in a virtual world. Subject feedback 
from their study suggest a strong desire for integration of \threed
visualizations, reaffirming the need to fuse techniques from SciVis and InfoVis.

%\textbf{Summary:} We take an approach similar to CarComVis [Seldmair2009], we
%generated a virtual vehicle based on the components mentioned in our dataset. However, rather 
%than using a coordinated views for information seeking, our interactions are more 
%tightly coupled with the \threed visualization. Also, our visualization style is
%based on the context of the dataset, rather than using a fixed visualization.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Focus+Context Techniques}
Focus+Context is a well known and practiced technique used to draw attention to
particular areas of interest while maintaining a certain degree of contextual 
information. There are various ways discriminate these areas, for example 
Hauser\cite{Hasuser2006} reviewed the general approaches where usage of colour, 
spatial-location, opacity and other resources are demonstrated. One interesting 
use of the focus+context technique is the idea of using lenses, the lens traces 
back to a real world metaphor of a magnification lens, where the area directly 
under the lens are distorted and enlarged by the optics. Translating this into the 
computer science field, the lens acts as a specialised user interface widget,
where the data graphics under the lens widget undergoes a some type of transformation to 
represent different semantics. 

Fisheye Lens\cite{Sarkar1992} is a well known lens technique for enlarging
details on a \twod image, for example on digital maps and circuit diagrams. Much
like its real world counterpart, a fisheye lens technique creates an ultra-wide viewing perspective that
creates a distortion displacement under the lens. The amount of displacement 
is controlled via a drop-off function that determines the degree of enlargement 
and shrinkage. For example a Gaussian like function is used to enlarge the 
center area of the lens while shrinking the area near the lens circumference, 
allowing the readers to see the central portion in greater detail. There are a
few usability issues with fisheye lens, and the same issue applies to all lens
that undergoes geometric distortion: it is difficult to tell where the focus is,
because the lens itself occludes nearby objects that helps the readers to
understand the overall context.

NPRLens\cite{Neumann2007a} allows the user to interactively create images in
a non-photorealistic style in an object independent manner. NPRLens performs
image processing on the \twod projections of objects in \threed space. Graphical data points are first projected to screen space, collected and aggregated into higher level graphical constructs such as line 
segments and curves. These can then be further manipulated such as stretching, 
shrinking or changing the stroke styles of each line segments. NPRLens supports
many of the effects discussed in the NPR section above.

MagicLens\cite{Viega1996} is likely the first foray of the lens idea taken into
3 dimensional space, unlike the previously mentioned works, MagicLens works with 
viewing volumes rather than \twod image maps. The lens in this scenario slices
the scene into different frustum volumes, each volume is then rendered separately 
and then recomposed together to create the final scene. This technique allows 
people to create interesting effects, for example exposing the skeletal 
structures of layered objects.

Other than stylization and distortion techniques, the same lens metaphor can
also be used as a way to augment information seeking, in this context the lens 
is an exploration device that exposes hidden data points, or data points that 
cannot be displayed in a readable manner. Generally these cases arise
because of overlapping points, or there are too many points on the screen,
making labelling of all points impractical as they introduce too much visual
clutter. Excentric Labeling \cite{Fekete1999} uses the lens metaphor to deal
with densely populated data points on a \twod illustration. Data points are not
labelled, instead, when the lens is hovered over the locations of the data
points, line segments are extended from the data points outward towards the
circumference of the lens. The actual text labels and descriptions are drawn
along the circumference, allowing viewers to make the connection between
graphics and text. The lens itself is movable which allows for exploration of interesting areas. 
Extended Excentric Labeling \cite{Bertini2009} further extends on the idea by
creating additional interactions with the lens, dynamic overviews are shown
inside the lens as miniature graphs summarizing of objects under the lens. The
labels are scrollable to allow exploration of dense areas without overcrowding screen space, the labels and 
the extending lines are also altered to minimize crossings which can cause 
additional visual complexity. 

% Use this in the visual desin section instead
%As for labelling placement, Ali \etal in their 
%paper described and categorized different layout types \cite{Ali2005}, they
%introduce a way to calculate labels’ end-points in image space using an indexed colouring scheme.

While both Excentric and Extended Excetric Lens operate in \twod space, there
are some researches with that takes the same ideas into environments that are
composed of \threed objects. Sonnet \etal \cite{Sonnet2004} created a medical volume rendering system where the lens takes
the form of a \threed cursor, although the viewers cannot view into the
cursor itself, the cursor creates a spherical volume that pushes other volumes
away from the center, thus creating more space for labels and annotations, and
reduce the occlusion issues that comes naturally with \threed environment.
BrainGazer \cite{Bruckner2009} is a volumetric visualization application
that allows users to draw a query path onto a segmented volume data. The
path behaves as a visual query, that is, a query that can be specified via
visual inspection, rather than explicitly stating the query parameters. When the
query is issued/drawn, volume objects near the drawn path are collected, and
their semantic information are displayed in a user interface panel.


%\textbf{Summary:} Our approach to focus+context resembles a combination of
%approaches taken by BrainGazer, NPRLens and Excentric Labels. Like BrainGazer, we allow 
%people to form visual queries via indexing spatial location with different semantics, 
%though we take a proactive approach to highlight and link the text data back to 
%the visualization by combining ideas from NPRLens and Excentric Labels.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Design for Touch Interface}
As the cost of touch surface computing becomes cheaper, we have gradually seeing 
interactive surfaces introduced into public areas, office work environments and 
personal spaces. Interactive interfaces brings new possibilities of exchanging 
information in a walk-by scenario \cite{Vogel2004}, without the need
of peripheral hardware devices such as the mouse or keyboard. New challenges also arise in how 
best to interact with the touch surface. Wigdor and Wixon in their book touched 
upon the idea of natural user interfaces, in the sense that an interface design 
that feels intuitive to use rather than an organic process\cite{Wigdor2011}.
Other researchers looked into building touch interactions from a user-defined perspective 
by studying how people create their own gestures given a effect-cause
scenario \cite{Wobbrock2009}, as well how gestures relate to
different social context \cite{Hinrichs2011}.


%\textbf{Summary:} While our primary focus is about the visualization of
% entities in text.
%We imagine a walk-up-and-use scenario where a single person can use the large 
%screen display for impromptu exploration, or to use the display as presentation 
%aid for meetings. Much of our design has been guided by past classifications of 
%gestures like ones mentioned above, as well to keep our interactions quite simple 
%to discourage any complexities that might arise from a social setting \cite{Brignull2003}.

